#+TITLE: 深度神经网络
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="../css/main.css" />
#+HTML_LINK_UP: ./shallow.html
#+HTML_LINK_HOME: ./neural-network.html
#+OPTIONS: num:nil timestamp:nil ^:nil

#+BEGIN_EXAMPLE
  目前为止学习了只有一个单独隐藏层的神经网络的正向传播和反向传播，还有逻辑回归

  并且还学到了向量化，这在随机初始化权重时是很重要

  接下来要做的是把这些理念集合起来，这样就可以执行你自己的深度神经网络
#+END_EXAMPLE
* 深层神经网络
  复习下前面的内容：

  逻辑回归和一个隐藏层的神经网络，结构如下：

  #+ATTR_HTML: image :width 70% 
  [[file:../pic/7c1cc04132b946baec5487ba68242362.png]]

  注意，神经网络的层数是这么定义的： *从左到右，由0开始定义* ，比如上边右图 $x_1$ , $x_2$ , $x_3$ 这层是第 $0$ 层，这层右边的隐藏层是第 $1$ 层，由此类推。如下图左边是两个隐藏层的神经网络，右边是5个隐藏层的神经网络：

  #+ATTR_HTML: image :width 70% 
  [[file:../pic/be71cf997759e4aeaa4be1123c6bb6ba.png]]

  严格上来说逻辑回归也是一个一层的神经网络，而上边右图是一个深得多的模型：
  + 有一个隐藏层的神经网络，就是一个两层神经网络
  + 当算神经网络的层数时，不算输入层，只算 _隐藏层_ 和 _输出层_ 

  #+BEGIN_EXAMPLE
    但是在过去的几年中，研究者已经意识到有一些函数，只有非常深的神经网络能学会，而更浅的模型则办不到

    尽管对于任何给定的问题很难去提前预测到底需要多深的神经网络，可以先去尝试逻辑回归，尝试一层然后两层隐含层

    然后把隐含层的数量看做是另一个可以自由选择大小的超参数，再保留交叉验证数据上评估，或者用开发集来评估
  #+END_EXAMPLE

  现在来看下深度学习的符号定义：

  #+ATTR_HTML: image :width 70% 
  [[file:../pic/9927bcb34e8e5bfe872937fccd693081.png]]

  #+BEGIN_EXAMPLE
    上图是一个4层的神经网络，有 3 个隐藏层

    可以看到，第一层有5个神经元数目，第二层5个，第三层3个
  #+END_EXAMPLE

  用 $L$ 表示层数： $\mathbf{L} = 4$ , 输入层的索引为 $0$ ：
  + 第一个隐藏层 $n^{[1]} = 5$ 表示有5个隐藏神经元
  + 同理 $n^{[2]} = 5$ , $n^{[3]} = 3$ , $n^{[4]} = 1$ （输出单元为1）
  + 输入层 $n^{[0]} = n_x = 3$ 

  在不同层所拥有的神经元的数目，对于每层 $a^{[l]}$ 都用来记作 $l$ 层激活后结果

  #+BEGIN_EXAMPLE
    会在后面看到在正向传播时，最终能会计算出这个结果
  #+END_EXAMPLE

  + 通过用激活函数 $g$ 计算 $z^{[l]}$ ，激活函数也被索引为层数 $l$ , 然后用 $w^{[l]}$ 来记作在 $l$ 层计算值的 *权重*
  + 类似的也有 $b^{[l]}$
  + 输入的特征记作 $x$ ，但是 $x$ 同样也是 $0$ 层的激活函数，所以 $x = a^{[0]}$
  + 最后一层 $a^{[L]}$ 等于这个神经网络所预测的输出结果

* 前向传播

  #+BEGIN_EXAMPLE
    之前学习了构成深度神经网络的基本模块

    比如每一层都有前向传播步骤以及一个相反的反向传播步骤
  #+END_EXAMPLE

  前向传播：输入 $a^{[l-1]}$ ，输出 $a^{[l]}$ ，缓存为 $z^{[l]}$ ；从实现的角度来可以缓存下 $w^{[l]}$ 和 $b^{[l]}$ ，这样更容易在不同的环节中调用函数

  #+ATTR_HTML: image :width 70% 
  [[file:../pic/7cfc4b5fe94dcd9fe7130ac52701fed5.png]]

  前向传播的步骤：

  \begin{equation}
  z^{[l]} = W^{[l]} \cdot a^{[l-1]} + b^{[l]} \\ 
  a^{[l]} = g^{[l]}(z^{[l]}) 
  \end{equation}

  向量化实现过程：

  \begin{equation}
  Z^{[l]} = W^{[l]} \cdot A^{[l-1]} + b^{[l]} \\
  A^{[l]} = g^{[l]}(Z^{[l]})
  \end{equation} 

  前向传播需要喂入 $A^{[0]}$ 也就是 $X$ ，来初始化；初始化的是第一层的输入值。$a^{[0]}$ 对应于一个训练样本的输入特征，而 $A^{[0]}$ 对应于一整个训练样本的输入特征
  #+BEGIN_EXAMPLE
    所以这就是这条链的第一个前向函数的输入，重复这个步骤就可以从左到右计算前向传播 
  #+END_EXAMPLE

* 反向传播
  输入为 $\mathrm{d} a^{[l]}$ ，输出为 $\mathrm{d} a^{[l-1]}$ ，$\mathrm{d} w^{[l]}$ ,  $\mathrm{d} b^{[l]}$   

  #+ATTR_HTML: image :width 70% 
  [[file:../pic/c13d2a8fa258125a5398030c97101ee1.png]]

  反向传播的步骤：

  \begin{equation}
  \mathrm{d} z^{[l]} = \mathrm{d} a^{[l]} \ast g^{[l]^{'}}(z^{[l]}) 
  \end{equation} 

  \begin{equation}
  \mathrm{d} w^{[l]} =  \mathrm{d} z^{[l]} \cdot a^{[l-1]}
  \end{equation}

  \begin{equation}
  \mathrm{d} b^{[l]} =  \mathrm{d} z^{[l]} 
  \end{equation}

  \begin{equation}
  \mathrm{d} a^{[l-1]} =  w^{[l]T} \cdot \mathrm{d} z^{[l]}
  \end{equation}

  \begin{equation}
  \mathrm{d} z^{[l]} =  w^{[l+1]T}\mathrm{d} z^{[l+1]} \ast g^{[l]^{'}}(z^{[l]}) 
  \end{equation}

  #+BEGIN_EXAMPLE
  式子（5）由式子（4）带入式子（1）得到，前四个式子就可实现反向函数
  #+END_EXAMPLE

  向量化实现过程：

  \begin{equation}
  \mathrm{d} Z^{[l]} = \mathrm{d} A^{[l]} \ast g^{[l]^{'}}(Z^{[l]})
  \end{equation}

  \begin{equation}
  \mathrm{d} W^{[l]} =  \frac{1}{m}\mathrm{d} Z^{[l]} \cdot A^{[l-1]T}
  \end{equation}

  \begin{equation}
  \mathrm{d} b^{[l]} =  \frac{1}{m} np.sum(\mathrm{d} z^{[l]}, \text{axis} = 1, \text{keepdims} = True)
  \end{equation}

  \begin{equation}
  \mathrm{d} A^{[l-1]} =  W^{[l]T} \cdot \mathrm{d} Z^{[l]}
  \end{equation}

  总结：

  #+ATTR_HTML: image :width 70% 
  [[file:../pic/53a5b4c71c0facfc8145af3b534f8583.png]]

  + 第一层可能有一个 $ReLU$ 激活函数，第二层为另一个 $ReLU$ 激活函数，第三层可能是$sigmoid$ 函数（如果做二分类的话），输出值为 $\hat{y}$ ，用来计算损失
  + 这样就可以向后迭代进行反向传播求导来求 $\mathrm{d} w^{[3]}$ , $\mathrm{d} b^{[3]}$ , $\mathrm{d}w^{[2]}$ , $\mathrm{d} b^{[2]}$ , $\mathrm{d} w^{[1]}$ , $\mathrm{d} b^{[1]}$
  + 在计算的时候，缓存会把 $z^{[1]}$ , $z^{[2]}$ , $z^{[3]}$ 传递过来，然后回传 $\mathrm{d} a^{[2]}$ ,  $\mathrm{d} a^{[1]}$
  + 也可以计算 $\mathrm{d}a^{[0]}$ ，但实际不会使用它

  #+BEGIN_EXAMPLE
    这里讲述了一个三层网络的前向和反向传播
  #+END_EXAMPLE

* 深层网络中的前向传播
