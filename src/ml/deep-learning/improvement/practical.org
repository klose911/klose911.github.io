#+TITLE: 实践层面
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="../css/main.css" />
#+HTML_LINK_HOME: ./improvment.html
#+OPTIONS: num:nil timestamp:nil ^:nil

#+BEGIN_EXAMPLE
首先说说神经网络机器学习中的问题
#+END_EXAMPLE
* 训练，验证，测试集
  在配置训练、验证和测试数据集的过程中做出正确决策会在很大程度上帮助大家创建高效的神经网络。训练神经网络时，需要做出很多决策，例如：
  + 神经网络分多少层
  + 每层含有多少个隐藏单元
  + 学习速率是多少
  + 各层采用哪些激活函数

  #+ATTR_HTML: image :width 70%
  [[file:../pic/L2_week1_2.png]]


  创建新应用的过程中，不可能从一开始就准确预测出这些信息和其他超级参数。实际上，应用型机器学习是一个 *高度迭代* 的过程，通常在项目启动时：
  1. 会先有一个初步想法
     #+BEGIN_EXAMPLE
       比如构建一个含有特定层数，隐藏单元数量或数据集个数等等的神经网络
     #+END_EXAMPLE
  2. 然后编码，并尝试运行这些代码
  3. 通过运行和测试得到该神经网络或这些配置信息的运行结果，可能会根据输出结果重新完善自己的想法，改变策略，或者为了找到更好的神经网络不断迭代更新自己的方案。

  现如今，深度学习已经在自然语言处理，计算机视觉，语音识别以及结构化数据应用等众多领域取得巨大成功。结构化数据无所不包，从广告到网络搜索

  #+BEGIN_EXAMPLE
    其中网络搜索不仅包括网络搜索引擎，还包括购物网站，从所有根据搜索栏词条传输结果的网站

    再到计算机安全，物流，比如判断司机去哪接送货，范围之广，不胜枚举
  #+END_EXAMPLE

  从一个领域或者应用领域得来的直觉经验，通常无法转移到其他应用领域，最佳决策取决于所 *拥有的数据量* ，计算机配置中 *输入特征的数量* ，用 _GPU_ 训练还是 _CPU_ ，GPU和CPU的具体配置以及其他诸多因素 

  #+ATTR_HTML: image :width 70%
  [[file:../pic/97596da07232f85d566710ee7dd2f8ad.png]]

  目前为止，对于很多应用系统，即使是经验丰富的深度学习行家也不太可能一开始就预设出最匹配的超级参数，所以说，应用深度学习是一个典型的 *迭代* 过程，需要多次循环往复，才能为应用程序找到一个称心的神经网络，因此 *循环该过程的效率* 是决定项目进展速度的一个关键因素，而创建 *高质量* 的 _训练数据集_ ， _验证集_ 和 _测试集_ 也有助于提高循环效率 

  #+ATTR_HTML: image :width 70%
  [[file:../pic/072653e2e9402d2857bfcb7b9f783a5c.png]]

  假设这是 _训练数据_ ，用一个 _长方形_ 表示，通常会将这些数据划分成几部分，一部分作为 *训练集* ，一部分作为简单交叉验证集，有时也称之为 *验证集* ，最后一部分则作为 *测试集* 

  接下来开始对训练执行算法，通过验证集或简单交叉验证集选择最好的模型，经过充分验证，选定了最终模型，然后就可以在测试集上进行评估了，为了无偏评估算法的运行状况 
  + 在机器学习发展的 *小数据量* 时代，常见做法是将所有数据三七分，就是人们常说的 _70%验证集_ ， _30%测试集_
  + 如果没有明确设置验证集，也可以按照 _60%训练_ ， _20%验证_ 和 _20%测试集_ 来划分
    #+BEGIN_EXAMPLE
      这是前几年机器学习领域普遍认可的最好的实践方法
    #+END_EXAMPLE

  #+ATTR_HTML: image :width 70%
  [[file:../pic/ebbfb8514ff5a983f41e938d5870b79d.png]]

  #+BEGIN_EXAMPLE
    如果只有100条，1000条或者1万条数据，那么上述比例划分是非常合理的
  #+END_EXAMPLE

  但是在大数据时代，现在的数据量可能是百万级别，那么验证集和测试集占数据总量的比例会趋向于变得更小。因为验证集的目的就是验证不同的算法，检验哪种算法更有效，因此，验证集要足够大才能评估，比如2个甚至10个不同算法，并迅速判断出哪种算法更有效。可能不需要拿出20%的数据作为验证集

  #+ATTR_HTML: image :width 70%
  [[file:../pic/1224fb0922d5673c380e6dad9ded0b6b.png]]

  #+BEGIN_EXAMPLE
    比如有100万条数据，那么取1万条数据便足以进行评估，找出其中表现最好的1-2种算法

    同样地，根据最终选择的分类器，测试集的主要目的是正确评估分类器的性能

    所以，如果拥有百万数据，只需要1000条数据，便足以评估单个分类器，并且准确评估该分类器的性能

    假设有100万条数据，其中1万条作为验证集，1万条作为测试集，100万里取1万，比例是1%，即：训练集占98%，验证集和测试集各占1%

    对于数据量过百万的应用，训练集可以占到99.5%，验证和测试集各占0.25%，或者验证集占0.4%，测试集占0.1%
  #+END_EXAMPLE

  总结一下：
  + 在机器学习中，通常将样本分成训练集，验证集和测试集三部分
  + 数据集规模相对较小，适用传统的划分比例
  + 数据集规模较大的，验证集和测试集要小于数据总量的20%或10%

  #+BEGIN_EXAMPLE
  后面会给出如何划分验证集和测试集的具体指导
  #+END_EXAMPLE

  现代深度学习的另一个趋势是越来越多的人在 _训练_ 和 _测试集_ *分布不匹配* 的情况下进行训练

  #+BEGIN_EXAMPLE
    假设要构建一个用户可以上传大量图片的应用程序，目的是找出并呈现所有猫咪图片

    可能用户都是爱猫人士，训练集可能是从网上下载的猫咪图片

    而验证集和测试集是用户在这个应用上上传的猫的图片

    就是说，训练集可能是从网络上抓下来的图片。而验证集和测试集是用户上传的图片

    结果许多网页上的猫咪图片分辨率很高，很专业，后期制作精良

    而用户上传的照片可能是用手机随意拍摄的，像素低，比较模糊，这两类数据有所不同'
  #+END_EXAMPLE


  针对这种情况，根据经验，建议大家要确保验证集和测试集的数据来自同一分布

  #+BEGIN_EXAMPLE
    要用验证集来评估不同的模型，尽可能地优化性能。如果验证集和测试集来自同一个分布就会很好
  #+END_EXAMPLE

  #+ATTR_HTML: image :width 70%
  [[file:../pic/459ffc8998b35b259bc3bab4fd4fb44c.png]]

  但由于深度学习算法需要大量的训练数据，为了获取更大规模的训练数据集，可以采用当前流行的各种创意策略

  #+BEGIN_EXAMPLE
    例如，网页抓取，代价就是训练集数据与验证集和测试集数据有可能不是来自同一分布

    但只要遵循这个经验法则，就会发现机器学习算法会变得更快
  #+END_EXAMPLE

  最后一点，就算没有测试集也不要紧，测试集的目的是对最终所选定的神经网络系统做出无偏估计，如果不需要无偏估计，也可以不设置测试集。所以如果只有验证集，没有测试集，我们要做的就是，在训练集上训练，尝试不同的模型框架，在验证集上评估这些模型，然后迭代并选出适用的模型。因为验证集中已经涵盖测试集数据，其不再提供无偏性能评估。当然，如果你不需要无偏估计，那就再好不过了

  #+ATTR_HTML: image :width 70%
  [[file:../pic/658fc77a1ea588d5768dcd1a9a90761a.png]]

  在机器学习中，如果只有一个训练集和一个验证集，而没有独立的测试集，遇到这种情况，训练集还被人们称为训练集，而验证集则被称为测试集，不过在实际应用中，人们只是把测试集当成简单交叉验证集使用，并没有完全实现该术语的功能，因为他们把验证集数据过度拟合到了测试集中

  #+BEGIN_EXAMPLE
    如果某团队跟你说他们只设置了一个训练集和一个测试集，需要很谨慎

    是不是真的有训练验证集，因为他们把验证集数据过度拟合到了测试集中

    让这些团队改变叫法，改称其为“训练验证集”，而不是“训练测试集”，可能不太容易

    即便认为“训练验证集“在专业用词上更准确

    实际上，如果不需要无偏评估算法性能，那么这样是可以的
  #+END_EXAMPLE

  #+ATTR_HTML: image :width 70%
  [[file:../pic/02556a7f525c0ec016483644755c2231.png]]

  #+BEGIN_EXAMPLE
    搭建训练验证集和测试集能够加速神经网络的集成，也可以更有效地衡量算法地偏差和方差

    从而帮助更高效地选择合适方法来优化算法
  #+END_EXAMPLE
* 偏差，方差

  #+BEGIN_EXAMPLE
    几乎所有机器学习从业人员都期望深刻理解偏差和方差，这两个概念易学难精

    即使自己认为已经理解了偏差和方差的基本概念，却总有一些意想不到的新东西出现

    关于深度学习的误差问题，另一个趋势是对偏差和方差的权衡研究甚浅，

    可能听说过这两个概念，但深度学习的误差很少权衡二者，总是分别考虑偏差和方差，却很少谈及偏差和方差的权衡问题
  #+END_EXAMPLE


  #+ATTR_HTML: image :width 70%
  [[file:../pic/e0ec4205933b7c2a9eaed9fbaa8d4afc.png]]

  假设这就是数据集：
  + 如果给这个数据集拟合一条直线，可能得到一个逻辑回归拟合，但它并不能很好地拟合该数据，这是 *高偏差* _high bias_ 的情况，称为 *欠拟合* _underfitting_ 
  + 相反的如果拟合一个非常复杂的分类器，但这看起来也不是一种很好的拟合方式分类器 *方差较高* _high variance_ ，数据 *过度拟合*  _overfitting_ 
    #+BEGIN_EXAMPLE
      比如深度神经网络或含有隐藏单元的神经网络，可能就非常适用于这个数据集
    #+END_EXAMPLE
  + 在两者之间，可能还有一些像图中这样的，复杂程度适中，数据拟合适度的分类器，这个数据拟合看起来更加合理，称之为 *适度拟合* _just right_ 是介于过度拟合和欠拟合中间的一类


  #+ATTR_HTML: image :width 70%
  [[file:../pic/05ac08b96177b5d0aaae7b7bfea64f3a.png]]

  在这样一个只有 $x_1$ 和 $x_2$ 两个特征的二维数据集中，可以绘制数据，将偏差和方差可视化。在多维空间数据中，绘制数据和可视化分割边界无法实现，但可以通过几个指标，来研究偏差和方差

  #+ATTR_HTML: image :width 70%
  [[file:../pic/2efd9728b5f07f914903dde309167a5d.png]]

  沿用猫咪图片分类这个例子，左边一张是猫咪图片，右边一张不是。理解偏差和方差的两个关键数据是 *训练集误差* _Train set error_ 和 *验证集误差* _Dev set error_ 

  #+BEGIN_EXAMPLE
  为了方便论证，假设可以辨别图片中的小猫，用肉眼识别几乎是不会出错的
  #+END_EXAMPLE

  假定训练集误差是1%，为了方便论证，假定验证集误差是11%，可以看出训练集设置得非常好，而验证集设置相对较差，可能过度拟合了训练集，在某种程度上，验证集并没有充分利用交叉验证集的作用，像这种情况，称之为 *高方差* 

  #+BEGIN_EXAMPLE
    通过查看训练集误差和验证集误差，便可以诊断算法是否具有高方差

    也就是说衡量训练集和验证集误差就可以得出不同结论
  #+END_EXAMPLE

  假设训练集误差是15%，把训练集误差写在首行，验证集误差是16%，假设该案例中人的错误率几乎为0%，人们浏览这些图片，分辨出是不是猫。算法并没有在训练集中得到很好训练，如果训练数据的拟合度不高，就是数据欠拟合，就可以说这种算法 *偏差比较高* 。相反，它对于验证集产生的结果却是合理的，验证集中的错误率只比训练集的多了1%，所以这种算法偏差高，因为它甚至不能拟合训练集

  再举一个例子，训练集误差是15%，偏差相当高，但是，验证集的评估结果更糟糕，错误率达到30%，在这种情况下，我会认为这种算法偏差高，因为它在训练集上结果不理想，而且方差也很高，这是 *方差偏差都很糟糕* 的情况。

  最后一个例子，训练集误差是0.5%，验证集误差是1%，用户看到这样的结果会很开心，猫咪分类器只有1%的错误率， *偏差和方差都很低* 


  #+BEGIN_EXAMPLE
    这些分析都是基于假设预测的，假设人眼辨别的错误率接近0%

    一般来说，最优误差也被称为贝叶斯误差，所以，最优误差接近0%
  #+END_EXAMPLE

  如果最优误差或贝叶斯误差非常高，比如15%。再看看这个分类器（训练误差15%，验证误差16%），15%的错误率对训练集来说也是非常合理的，偏差不高，方差也非常低
  #+ATTR_HTML: image :width 70%
  [[file:../pic/c61d149beecddb96f0f93944320cf639.png]]

  #+BEGIN_EXAMPLE
    当所有分类器都不适用时，如何分析偏差和方差呢？

    比如，图片很模糊，即使是人眼，或者没有系统可以准确无误地识别图片
  #+END_EXAMPLE

  在这种情况下，最优误差会更高，那么分析过程就要做些改变了：
  + 重点是通过查看训练集误差，可以判断数据拟合情况，至少对于训练数据是这样，可以判断是否有偏差问题，然后查看错误率有多高
  + 当完成训练集训练，开始使用验证集验证时，可以判断方差是否过高，从训练集到验证集的这个过程中，可以判断方差是否过高

  #+ATTR_HTML: image :width 70%
  [[file:../pic/fcdc2a7afbaaa07ca08b078bb4d37e8d.png]]

  #+BEGIN_EXAMPLE
    以上分析的前提都是假设基本误差很小，训练集和验证集数据来自相同分布

    如果没有这些假设作为前提，分析过程更加复杂，接下来将会讨论
  #+END_EXAMPLE

  偏差和方差都高是什么样子呢？这种情况对于两个衡量标准来说都是非常糟糕的
  #+ATTR_HTML: image :width 70%
  [[file:../pic/f8fdbf9ed01b8634573125e0fb2ca860.png]]

  这样的分类器，会产生高偏差，因为它的数据拟合度低，像这种接近线性的分类器，数据拟合度低 
  #+ATTR_HTML: image :width 70%
  [[file:../pic/e9451f36e8baa41b74c95d9a09b0f028.png]]

  但是如果稍微改变一下分类器，这里用紫色笔画出，它会过度拟合部分数据，用紫色线画出的分类器具有高偏差和高方差，偏差高是因为它几乎是一条线性分类器，并未拟合数据

  #+ATTR_HTML: image :width 70%
  [[file:../pic/6e86aa7d9b21b1a49bf4a084c7503527.png]]

  这种二次曲线能够很好地拟合数据：

  #+ATTR_HTML: image :width 70%
  [[file:../pic/67dc5997a956314e238e6fc362f9883d.png]]

  这条曲线中间部分灵活性非常高，却过度拟合了这两个样本，这类分类器偏差很高，因为它几乎是线性的

  #+ATTR_HTML: image :width 70%
  [[file:../pic/6bf7e092ddf93104ea64a5ddecbb7c6d.png]]

  而采用曲线函数或二次元函数会产生高方差，因为它曲线灵活性太高以致拟合了这两个错误样本和中间这些活跃数据

  #+BEGIN_EXAMPLE
    这看起来有些不自然，从两个维度上看都不太自然

    但对于高维数据，有些数据区域偏差高，有些数据区域方差高

    所以在高维数据中采用这种分类器看起来就不会那么牵强了
  #+END_EXAMPLE

  总结一下：
  + 如何通过分析在训练集上训练算法产生的误差和验证集上验证算法产生的误差来诊断算法是否存在高偏差和高方差，是否两个值都高，或者两个值都不高
  + 根据算法偏差和方差的具体情况决定接下来你要做的工作

  #+BEGIN_EXAMPLE
    接下来会根据算法偏差和方差的高低情况讲解一些机器学习的基本方法，更系统地优化算法
  #+END_EXAMPLE
* 机器学习基础
  下图就是在训练神经网络用到的基本方法：

  #+ATTR_HTML: image :width 70%
  [[file:../pic/L2_week1_8.png]]

  初始模型训练完成后，首先要知道算法的 *偏差高不高* ：
  + 如果偏差较高，试着评估训练集或训练数据的性能
  + 如果偏差的确很高，甚至无法拟合训练集，那么要做的就是选择一个新的网络：
    + 比如含有更多隐藏层或者隐藏单元的网络
    + 或者花费更多时间来训练网络
    + 或者尝试更先进的优化算法

  #+BEGIN_EXAMPLE
    以后会看到许多不同的神经网络架构，或许能找到一个更合适解决此问题的新的网络架构

    必须去尝试，可能有用，也可能没用。不过采用规模更大的网络通常都会有所帮助

    延长训练时间不一定有用，但也没什么坏处

    直到解决掉偏差问题，这是最低标准，反复尝试，至少能够拟合训练集 
  #+END_EXAMPLE

  如果网络足够大，通常可以很好的拟合训练集，只要能扩大网络规模，如果图片很模糊，算法可能无法拟合该图片，但如果有人可以分辨出图片，如果觉得基本误差不是很高，那么训练一个更大的网络，就应该可以，至少可以很好地拟合训练集或者过拟合训练集。一旦偏差降低到可以接受的数值，检查一下 *方差有没有问题* ，为了评估方差，要查看验证集性能，从一个性能理想的训练集推断出验证集的性能是否也理想，如果方差高：
  + 最好的解决办法就是 *采用更多数据* ，如果能做到，会有一定的帮助
  + 但有时候，无法获得更多数据，也可以尝试通过 _正则化_ 来减少过拟合

  如果能找到更合适的神经网络框架，有时它可能会一箭双雕，同时减少方差和偏差

  #+BEGIN_EXAMPLE
    如何实现呢？想系统地说出做法很难

    总之就是不断重复尝试，直到找到一个低偏差，低方差的框架，这时就成功了
  #+END_EXAMPLE

  有两点需要大家注意：
  1. 高偏差和高方差是两种不同的情况，后续要尝试的方法也可能完全不同，通常会用训练验证集来诊断算法是否存在偏差或方差问题，然后根据结果选择尝试部分方法
     #+BEGIN_EXAMPLE
       举个例子，如果算法存在高偏差问题，准备更多训练数据其实也没什么用处，至少这不是更有效的方法

       所以大家要清楚存在的问题是偏差还是方差，还是两者都有问题，明确这一点有助于选择出最有效的方法
     #+END_EXAMPLE
  2. 在机器学习的初期阶段，关于所谓的偏差方差权衡的讨论屡见不鲜，原因是能尝试的方法有很多。可以增加偏差，减少方差，也可以减少偏差，增加方差，但是在深度学习的早期阶段，我们没有太多工具可以做到只减少偏差或方差却不影响到另一方。但在当前的深度学习和大数据时代，只要持续训练一个更大的网络，只要准备了更多数据，那么也并非只有这两种情况
     #+BEGIN_EXAMPLE
       假定只要正则适度，通常构建一个更大的网络便可以，在不影响方差的同时减少偏差

       而采用更多数据通常可以在不过多影响偏差的同时减少方差

       现在有工具可以做到在减少偏差或方差的同时，不对另一方产生过多不良影响

       这就是深度学习对监督式学习大有裨益的一个重要原因，也是不用太过关注如何平衡偏差和方差的一个重要原因

       但有时有很多选择，减少偏差或方差而不增加另一方。最终，会得到一个非常规范化的网络
     #+END_EXAMPLE

* 正则化
  深度学习可能存在 _过拟合_ 问题 *高方差* ，有两个解决方法：
  + 正则化
  + 准备更多的数据

  #+BEGIN_EXAMPLE
    如果怀疑神经网络过度拟合了数据，即存在高方差问题，那么最先想到的方法可能是正则化

    另一个解决高方差的方法就是准备更多数据，这也是非常可靠的办法

    但可能无法时时准备足够多的训练数据，或者，获取更多数据的成本很高

    而正则化有助于避免过度拟合，或者减少网络误差
  #+END_EXAMPLE
** L2 正则化
   当用逻辑回归来实现这些设想，就是求成本函数 $J$ 的最小值，它是定义的成本函数，参数包含一些训练数据和不同数据中个体预测的损失， $w$ 和 $b$ 是逻辑回归的两个参数， $w$ 是一个多维度参数矢量， $b$ 是一个实数。在逻辑回归函数中加入正则化，只需添加参数 $\lambda$ ，也就是正则化参数

   \begin{equation}
   J(w, b） = \frac{1}{m}\sum_{i=1}^{m}L(y^{\hat{(i)}}, y^{(i)}) + \frac{\lambda}{2m}\|w\|_{2}^{2} \\ 
   \|w\|_{2}^{2} = \sum_{j=1}^{n_x}w_j^2 = w^{T}w 
   \end{equation}

   + $\frac{\lambda}{2m}$ 乘以 $w$ 欧几里德范数的平方
   + $w$ 欧几里德范数的平方等于 $w_j$ （$j$ 值从 $1$ 到 $n_x$ ）平方的和，也可表示为 $w^{T}w$ ，也就是向量参数 $w$ 的欧几里德范数（2范数）的平方
   + 此方法称为 $L2$ 正则化。因为这里用了欧几里德法线，被称为向量参数 $w$ 的 $L2$ 范数 

   为什么只正则化参数？为什么不再加上参数 $w$ 呢？ 可以这么做，只是我习惯省略不写，因为 $w$ 通常是一个高维参数矢量，已经可以表达高偏差问题， $w$ 可能包含有很多参数，而 $b$ 只是单个数字，所以 $w$ 几乎涵盖所有参数，如果加了参数 $b$ ，其实也没太大影响，因为 $b$ 只是众多参数中的一个，所以通常省略不计，如果你想加上这个参数，完全没问题 

   #+ATTR_HTML: image :width 70%
   [[file:../pic/84c4e19130a91a09120087dd704bbaa4.png]]

   $L2$ 正则化是最常见的正则化类型，还有 $L1$ 正则化：
   + 加的不是 $L2$ 范数，而是正则项 $\frac{\lambda}{m}$ 乘以 $\sum_{j=1}^{n_x}|w|$
   + $\sum_{j=1}^{n_x}|w|$ 也被称为参数 $w$ 向量的 $L1$ 范数
   + 无论分母是 $m$ 还是 $2m$ ，它都是一个比例常量 

   \begin{equation}
   J(w, b） = \frac{1}{m}\sum_{i=1}^{m}L(y^{\hat{(i)}}, y^{(i)}) + \frac{\lambda}{m}\sum_{j=1}^{n_x}|w|
   \end{equation}

   如果用的是 $L1$ 正则化， $w$ 最终会是 *稀疏* 的，也就是说 $w$ 向量中有很多0

   #+BEGIN_EXAMPLE
     有人说这样有利于压缩模型，因为集合中参数均为0，存储模型所占用的内存更少

     实际上，虽然L1正则化使模型变得稀疏，却没有降低太多存储内存

     所以我认为这并不是L1正则化的目的，至少不是为了压缩模型

     人们在训练网络时，越来越倾向于使用L2正则化
   #+END_EXAMPLE


   最后一个细节， $\lambda$ 是正则化参数，通常使用验证集或交叉验证集来配置这个参数，尝试各种各样的数据，寻找最好的参数，要考虑训练集之间的权衡，把参数设置为较小值，这样可以避免过拟合，所以 $\lambda$ 是另外一个需要调整的超参数，顺便说一下，为了方便写代码

   #+BEGIN_EXAMPLE
     在Python编程语言中，lambda是一个保留字段

     编写代码时，我们删掉a，写成lambdd，以免与Python中的保留字段冲突
   #+END_EXAMPLE

   如何在神经网络中实现 $L2$ 正则化呢？ 
   #+ATTR_HTML: image :width 70%
   [[file:../pic/2e88bd6f30a8ff8014d6d7dbe6d0488a.png]]

   神经网络含有一个成本函数，该函数包含 $W^{[1]}$ , $b^{[1]}$ 到 $W^{[l]}$ , $b^{[l]}$ 所有参数，字母 $L$ 是神经网络所含的层数，因此成本函数等于 $m$ 个训练样本损失函数的总和乘以 $\frac{1}{m}$ ，正则项为 $\frac{\lambda}{2m}\sum_{1}^{L}|W^{[l]}|^2$ ， $\|W^{[l]}\|^2$ 为范数平方，被定义为矩阵中所有元素的平方求和 


   \begin{equation}
   J(w^{[1]}, b^{[1]},\ldots, w^{[L]}, b^{[L]}) = \frac{1}{m}\sum_{i=1}^{m}L(y^{\hat{(i)}}, y^{(i)}) + \frac{\lambda}{2m}\sum_{l=1}^{L}\|w^{[l]}\|_{F}^{2} \\ 
   \|w^{[l]}\|_{F}^{2} = \sum_{i=1}^{n^{[l-1]}}\sum_{j=1}^{n^{[l]}}(w_{ij}^{[l]})^2 
   \end{equation}

   再看下求和公式的具体参数，第一个求和符号其值 $i$ 从 $1$ 到 $n^{[l-1]}$ ，第二个其值 $j$ 从 $1$ 到 $n^{[l]}$ ：
   + $W$ 是一个的 $n^{[l]} \times n^{[l-1]}$ 多维矩阵
   + $n^{[l]}$ 表示$l$ 层单元的数量，$n^{[l-1]}$ 表示第 $l-1$ 层隐藏单元的数量

   #+ATTR_HTML: image :width 70%
   [[file:../pic/5663bd9360df02b7e5a04c01d4e1bbc7.png]]

   该矩阵范数被称作 _弗罗贝尼乌斯范数_ ，用下标 $F$ 标注

   #+BEGIN_EXAMPLE
     鉴于线性代数中一些神秘晦涩的原因，不称之为“矩阵L2范数”，而称它为“弗罗贝尼乌斯范数”

     它表示一个矩阵中所有元素的平方和
   #+END_EXAMPLE

   又该如何使用该范数实现梯度下降呢？以前用向后传播计算出的 $\mathrm{d}W$ 值，也就是 $J$ 对 $W$ 的偏导数，这里实际上是把 $W$ 替换为 $W^{[l]}$ 

   #+ATTR_HTML: image :width 70%
   [[file:../pic/b6856a371ed552fd6b9ada2068ab4c2c.png]]

   现在额外增加了正则化项，要做的就是给 $\mathrm{d}W$ 加上这一项 $\frac{\lambda}{m}W^{[l]}$ ，然后计算这个更新项，来获取新的 $\mathrm{d}W^{[l]}$ ，它的定义含有相关参数代价函数导数和，以及最后添加的额外正则项
   #+ATTR_HTML: image :width 70%
   [[file:../pic/dafb163da5b9c3ece677a7ebce05b680.png]]

   $W^{[l]}$ 被更新为 $W^{[l]}$ 减去学习率 $\alpha$ 乘以 backprop 和 $\frac{\lambda}{m}W^{[l]}$ 
   #+ATTR_HTML: image :width 70%
   [[file:../pic/f752bc74e0978320a72bcb15d1777cf8.png]]

   该正则项说明，不论 $W^{[l]}$ 是什么，都试图让它变得更小，实际上，相当于给矩阵 $W$ 乘以 $1-\alpha\frac{\lambda}{m}$ 倍的权重。它就像一般的梯度下降，$W$ 被更新为少了 $\alpha$ 乘以backprop输出的最初梯度值，同时 $W$ 也乘以了这个系数，而这个系数小于1，因此正则化也被称为 _权重衰减_ 
   #+ATTR_HTML: image :width 70%
   [[file:../pic/cba0f1c7a480139acb04e762e4fe57f8.png]]

** 为什么正则化有利于预防过拟合呢? 
   #+BEGIN_EXAMPLE
     为什么正则化有利于预防过拟合呢？为什么它可以减少方差问题？

     通过两个例子来直观体会一下
   #+END_EXAMPLE

   #+ATTR_HTML: image :width 70%
   [[file:../pic/15d5607b8ac1c1fbaa023cb6060633f2.png]]

   左图是高偏差，右图是高方差，中间是 *Just Right* 

   #+ATTR_HTML: image :width 70%
   [[file:../pic/d5ee6f2b60ff7601d50967f4365d0ecb.png]]

   当添加正则项之后，它可以 *避免数据权值矩阵过大* ，直观上理解就是如果正则化 $\lambda$ 设置得足够大，权重矩阵 $W$ 被设置为接近于0的值。于是基本上消除了这些隐藏单元的许多影响。如果是这种情况，这个被大大简化了的神经网络会变成一个很小的网络，小到如同一个逻辑回归单元，可是深度却很大，它会使这个网络从过度拟合的状态更接近左图的高偏差状态

   #+ATTR_HTML: image :width 70%
   [[file:../pic/2aafa244c3f184cc271b26d1d95d70c9.png]]

   但是 $\lambda$ 会存在一个中间值，于是会有一个接近“Just Right”的中间状态 

   #+BEGIN_EXAMPLE
     实际上是该神经网络的所有隐藏单元依然存在，但是它们的影响变得更小了

     神经网络变得更简单了，貌似这样更不容易发生过拟合

     虽然不确定这个直觉经验是否有用，不过在编程中执行正则化时，会看到一些方差减少的结果
   #+END_EXAMPLE

   再来直观感受一下，正则化为什么可以预防过拟合，假设用的是这样的双曲线激活函数
   #+ATTR_HTML: image :width 70%
   [[file:../pic/f35d94efc03123e4a5de6496c1b896c0.png]]

   用 $g(z)$ 表示 $tanh(z)$ , 那么$z$ 只要非常小，并且只涉及少量参数，就体现了双曲正切函数的 *线性* 状态，当 $z$ 扩展为更大值或者更小值，激活函数开始变得非线性
   #+ATTR_HTML: image :width 70%
   [[file:../pic/884d5768243f4badc77356f843cb8c0c.png]]

   如果正则化参数 $\lambda$ 很大，则激活函数的参数会相对较小

   #+ATTR_HTML: image :width 70%
   [[file:../pic/63fa65e2609fe7a6909544247855d1a5.png]]

   如果 $W$ 变小，相对来说， $z$ 也会变小：
   #+ATTR_HTML: image :width 70%
   [[file:../pic/fcdff0820223fb8a5756ad0bf99991fe.png]]

   特别是，如果的 $z$ 值最终在这个范围内， $g(z)$ 都是相对较小的值，大致呈线性，每层几乎都是线性的，和线性回归函数一样：

   #+ATTR_HTML: image :width 70%
   [[file:../pic/8248be8e83121535b73969a4599fbb08.png]]

   如果每层都是线性的，那么整个网络就是一个线性网络，即使是一个非常深的深层网络，都具有线性激活函数的特征，最终只能计算线性函数

   #+ATTR_HTML: image :width 70%
   [[file:../pic/307675386a412e57cf60be3381f02a64.png]]

   总结：应用之前定义的 $J$ 代价函数，增加了一项，目的是预防权重过大
   + 如果正则化参数 $\lambda$ 变得很大， $W$ 参数会很小， $z$ 也会相对变小，此时忽略 $b$ 的影响
   + 实际上， 如果 $z$ 的取值范围很小，这个激活函数，也就是曲线函数 $tanh$ 会相对呈线性，整个神经网络会计算离线性函数近的值
   + 这个线性函数非常简单，并不是一个极复杂的高度非线性函数，不会发生过拟合

   #+ATTR_HTML: image :width 70%
   [[file:../pic/b84cc50eb967cd177007e21e93960b5c.png]]

   如果使用的是梯度下降函数，在调试梯度下降时，其中一步就是把代价函数 $J$ 设计成这样一个函数，在调试梯度下降时，它代表梯度下降的调幅数量。可以看到，代价函数对于梯度下降的每个调幅都单调递减

   #+BEGIN_EXAMPLE
     这就是正则化，它是在训练深度学习模型时最常用的一种方法

     在深度学习中，还有一种方法也用到了正则化，就是dropout正则化
   #+END_EXAMPLE

**  dropout 正则化
   除了正则化，还有一个非常实用的正则化方法： *DropOut* _随机失活_ 

   #+ATTR_HTML: image :width 50%
   [[file:../pic/97e37bf0d2893f890561cda932ba8c42.png]]

   假设在训练上图这样的神经网络，它存在过拟合，这就是dropout所要处理的，复制这个神经网络，dropout会遍历网络的每一层，并设置消除神经网络中节点的概率。假设网络中的每一层，每个节点都以抛硬币的方式设置概率，每个节点得以保留和消除的概率都是0.5，设置完节点概率，会消除一些节点，然后删除掉从该节点进出的连线，最后得到一个节点更少，规模更小的网络，然后用backprop方法进行训练

   #+ATTR_HTML: image :width 50%
   [[file:../pic/e45f9a948989b365650ddf16f62b097e.png]]


   #+ATTR_HTML: image :width 50%
   [[file:../pic/9fa7196adeeaf88eb386fda2e9fa9909.png]]

   这是网络节点精简后的一个样本，对于其它样本，照旧以抛硬币的方式设置概率，保留一类节点集合，删除其它类型的节点集合。对于每个训练样本，都将采用一个精简后神经网络来训练它，这种方法似乎有点怪，单纯遍历节点，编码也是随机的，可它真的有效

   #+BEGIN_EXAMPLE
     可想而知，针对每个训练样本训练规模极小的网络

     最后可能会认识到为什么要正则化网络，因为在训练极小的网络
   #+END_EXAMPLE

   #+ATTR_HTML: image :width 70%
   [[file:../pic/70b248490e496fed9b8d1d616e4d8303.png]]

   如何实施dropout呢？方法有几种，最常用的方法，即 *inverted dropout* _反向随机失活_ ，出于完整性考虑，用一个三层 $l=3$ 网络来举例说明

   #+BEGIN_EXAMPLE
     编码中会有很多涉及到3的地方

     这里只举例说明如何在某一层中实施dropout
   #+END_EXAMPLE

   首先要定义向量 $d$ ， $d^{[3]}$ 表示一个三层的 $dropout$ 向量：

   #+BEGIN_SRC python 
  d3 = np.random.rand(a3.shape[0],a3.shape[1])
   #+END_SRC

   然后看它是否小于某数，这被称之为 *keep-prob* ，keep-prob是一个具体数字，它表示保留某个隐藏单元的概率，它的作用就是生成随机矩阵

   #+BEGIN_EXAMPLE
   上个示例中它是0.5，而本例中它是0.8 

   它意味着消除任意一个隐藏单元的概率是0.2
   #+END_EXAMPLE

   $d^{[3]}$ 是一个矩阵，每个样本和每个隐藏单元，其中 $d^{[3]}$ 中的每个元素值只能是1或0， 为1的概率都是0.8，对应为0的概率是0.2

   #+BEGIN_EXAMPLE
   如果对 $a^{[3]} 进行因子分解，效果也是一样的
   #+END_EXAMPLE

   接下来要做的就是从第三层中计算新的激活函数值，这里叫它 $a^{[3]}$ ，等于以前计算的激活函数值乘以 $d^{[3]}$

   #+BEGIN_SRC python 
  a3 =np.multiply(a3,d3)
   #+END_SRC

   这里是元素相乘，也可写为 $a3 \ast= d3$ ，它的作用就是让 $d^{[3]}$ 中0元素与 $a^{[3]}$ 中相对元素归零 

   #+ATTR_HTML: image :width 70%
   [[file:../pic/c75fe55c6bc17b60b00f5360aab180f4.png]]

   #+BEGIN_EXAMPLE
     如果用python实现该算法的话，d3 则是一个布尔型数组，值为true和false，而不是1和0

     乘法运算依然有效，python会把true和false翻译为1和0，可以用python尝试一下
   #+END_EXAMPLE
   最后，向外扩展 $a^{[3]}$ ，用它除以0.8，或者除以 $keep-prob$ 参数 

   #+ATTR_HTML: image :width 70%
   [[file:../pic/6ba17ffcb3ff22a4a0ec857e66946086.png]]

   下面解释一下为什么要这么做，为方便起见，假设第三隐藏层上有50个单元或50个神经元，在一维 $a^{[3]}$ 上是50，通过因子分解将它拆分成 $50 \times m$ 维的，保留和删除它们的概率分别为80%和20%，这意味着最后被删除或归零的单元平均有10 $50 \times 20%=10$ 个，现在看下 $z^{[4]} = w^{[4]}a^{[3]} + b^{[4]}$ ，预期是 $a^{[3]}$ 减少20%，也就是说 $a^{[3]}$ 中有20%的元素被归零，为了不影响 $z^{[4]}$ 的期望值，需要用 $w^{[4]}\cdot\frac{a^{[3]}}{0.8}$ ，它将会修正或弥补所需的那20%，划线部分就是所谓的 *dropout* 方法 

   #+ATTR_HTML: image :width 70%
   [[file:../pic/272d902720be9993454c6d9a5f0bec49.png]]


   如果keep-prop设置为1，那么就不存在dropout，因为它会保留所有节点。反向随机失活（inverted dropout）方法通过除以keep-prob，确保 $a^{[3]}$ 的期望值不变 


   #+BEGIN_EXAMPLE
     事实证明，在测试阶段，当评估一个神经网络时，也就是用绿线框标注的反向随机失活方法，使测试阶段变得更容易，因为它的数据扩展问题变少

     目前实施dropout最常用的方法就是Inverted dropout，建议大家动手实践一下

     Dropout早期的迭代版本都没有除以keep-prob，所以在测试阶段，平均值会变得越来越复杂，不过那些版本已经不再使用了
   #+END_EXAMPLE

   现在使用的是 $d$ 向量，会发现，不同的训练样本，清除不同的隐藏单元也不同。实际上，如果通过相同训练集多次传递数据，每次训练数据的梯度不同，则随机对不同隐藏单元归零，有时却并非如此

   #+BEGIN_EXAMPLE
     比如，需要将相同隐藏单元归零，第一次迭代梯度下降时，把一些隐藏单元归零

     第二次迭代梯度下降时，也就是第二次遍历训练集时，对不同类型的隐藏层单元归零

     向量d 或 d^3 用来决定第三层中哪些单元归零，无论用foreprop还是backprop，这里只介绍了foreprob
   #+END_EXAMPLE

   用 $a^{[0]}$ 第0层的激活函数标注为测试样本 $x$ ，在测试阶段不使用dropout函数，尤其是像下列情况：

   \begin{equation} 
   z^{[1]} = w^{[1]}a^{[0]} + b^{[1]} \\ 
   a^{[1]} = g^{[1]}(z^{[1]}) \\ 
   z^{[2]} = w^{[2]}a^{[1]} + b^{[2]} \\
   a^{[2]} = g^{[2]}(z^{[2]}) \\ 
   \ldots  
   \end{equation} 

   以此类推直到最后一层，预测值为 $\hat{y}$ 
 
   #+ATTR_HTML: image :width 70%
   [[file:../pic/de49d4e160f29544055819c5ab1dd9c0.png]]

   显然在测试阶段，并未使用 $dropout$ ，自然也就不用抛硬币来决定失活概率，以及要消除哪些隐藏单元了，因为在测试阶段进行预测时，不期望输出结果是随机的，如果测试阶段应用dropout函数，预测会受到干扰

   #+BEGIN_EXAMPLE
     理论上，只需要多次运行预测处理过程，每一次，不同的隐藏单元会被随机归零

     预测处理可以遍历它们，但计算效率低，得出的结果也几乎相同，与这个不同程序产生的结果极为相似
   #+END_EXAMPLE

   $Inverted dropout$ 函数在除以 $keep-prob$ 时可以记住上一步的操作，目的是确保即使在测试阶段不执行dropout来调整数值范围，激活函数的预期结果也不会发生变化，所以没必要在测试阶段额外添加尺度参数，这与训练阶段不同 

*** 理解 dropout
    #+BEGIN_EXAMPLE
      Dropout可以随机删除网络中的神经单元，为什么可以通过正则化发挥如此大的作用呢？
    #+END_EXAMPLE

    直观上理解：不要依赖于任何一个特征，因为该单元的输入可能随时被清除
    #+ATTR_HTML: image :width 70%
    [[file:../pic/L2_week1_16.png]]

    从单个神经元入手，如图，这个单元的工作就是输入并生成一些有意义的输出。通过 $dropout$ ，该单元的输入几乎被消除，有时这两个单元会被删除，有时会删除其它单元，就是说，用 _紫色_ 圈起来的这个单元，它不能依靠任何特征，因为特征都有可能被随机清除，或者说该单元的输入也都可能被随机清除。不愿意把所有赌注都放在一个节点上，不愿意给任何一个输入加上太多权重，因为它可能会被删除，因此该单元通过这种方式传播下去，并为单元的四个输入增加一点权重，通过传播所有权重， $dropout$ 将产生收缩权重的平方范数的效果

    #+BEGIN_EXAMPLE
      和之前讲的L2 正则化类似；实施dropout的结果是它会压缩权重，并完成一些预防过拟合的外层正则化

      而 L2 对不同权重的衰减是不同的，它取决于激活函数倍增的大小
    #+END_EXAMPLE

    总结： $dropout$ 的功能类似于 $L2$ 正则化，与 $L2$ 正则化不同的是应用方式不同会带来一点点小变化，甚至更适用于不同的输入范围 

    #+ATTR_HTML: image :width 70%
    [[file:../pic/9b9f963a73e3ef9fcac008b179b6cf74.png]]

    实施 $dropout$ 的另一个细节是，这是一个拥有三个输入特征的网络，其中一个要选择的参数是 $keep-prob$ ，它代表每一层上保留单元的概率。所以不同层的 $keep-prob$ 也可以变化：
    + 第一层 $W^{[1]}$ 矩阵是 $7 \times 3$
    + 第二个权重矩阵 $W^{[2]}$ 是 $7 \times 7$
    + 第三个权重矩阵 $W^{[3]}$ 是 $3 \times 7$ 

    $W^{[2]}$ 是最大的权重矩阵，因为拥有最大参数集。为了预防矩阵的过拟合，对于这一层，也就是第二层，它的 $keep-prob$ 值应该相对较低，比如是0.5。对于其它层，过拟合的程度可能没那么严重，它们的 $keep-prob$ 值可能高一些，比如是0.7。如果在某一层，不必担心其过拟合的问题，那么 $keep-prob$ 可以为1

    #+BEGIN_EXAMPLE
      为了表达清除，用紫色线笔把它们圈出来，每层keep-prob的值可能不同
    #+END_EXAMPLE

    #+ATTR_HTML: image :width 70%
    [[file:../pic/5fcb1441866d6d3eff1164ed4ea38cfe.png]]

    注意 $keep-prob$ 的值是1，意味着保留所有单元，并且不在这一层使用dropout，对于有可能出现过拟合，且含有诸多参数的层，可以把keep-prob设置成比较小的值，以便应用更强大的dropout，有点像在处理正则化的正则化参数 $\lambda$ ，尝试对某些层施行更多正则化

    #+BEGIN_EXAMPLE
      从技术上讲，也可以对输入层应用dropout，有机会删除一个或多个输入特征，虽然现实中通常不这么做，keep-prob的值为1，是非常常用的输入值，也可以是0.9

      但是消除一半的输入特征是不太可能的，如果遵守这个准则，keep-prob会接近于1，即使你对输入层应用dropout
    #+END_EXAMPLE

    总结一下，如果担心某些层比其它层更容易发生过拟合：
    + 可以把某些层的keep-prob值设置得比其它层更低，缺点是为了使用交叉验证，要搜索更多的超级参数
    + 另一种方案是在一些层上应用dropout，而有些层不用dropout，应用dropout的层只含有一个超级参数，就是keep-prob 

    #+BEGIN_EXAMPLE
      实施dropout，在计算机视觉领域有很多成功的第一次

      计算视觉中的输入量非常大，输入太多像素，以至于没有足够的数据，所以dropout在计算机视觉中应用得比较频繁，有些计算机视觉研究人员非常喜欢用它，几乎成了默认的选择

      但要牢记一点，dropout是一种正则化方法，它有助于预防过拟合，因此除非算法过拟合，不然是不需要使用dropout的

      所以它在其它领域应用得比较少，主要存在于计算机视觉领域，因为通常没有足够的数据，所以一直存在过拟合

      这就是有些计算机视觉研究人员如此钟情于dropout函数的原因。直观上不能概括其它学科
    #+END_EXAMPLE

    #+ATTR_HTML: image :width 70%
    [[file:../pic/13987e28fa537a110c6b123fc4455f7c.png]]

    $dropout$ 一大缺点就是代价函数 $J$ 不再被明确定义，每次迭代，都会随机移除一些节点，如果反复检查梯度下降的性能，实际上是很难进行复查的。因为所优化的代价函数实际上并没有明确定义，或者说在某种程度上很难计算，所以失去了调试工具来绘制这样的图片

    #+BEGIN_EXAMPLE
      通常会关闭dropout函数，将keep-prob的值设为1，运行代码，确保J函数单调递减

      然后打开dropout函数，希望在dropout过程中，代码并未引入bug

      虽然并没有关于这些方法性能的数据统计，但可以把它们与dropout方法一起使用
    #+END_EXAMPLE

** 其他正则化方法
