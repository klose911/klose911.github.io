<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>浅层神经网络</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Wu, Shanliang" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="../css/main.css" />
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2019 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="./basics.html"> UP </a>
 |
 <a accesskey="H" href="./neural-network.html"> HOME </a>
</div><div id="content">
<h1 class="title">浅层神经网络</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org773ead7">神经网络概述</a></li>
<li><a href="#orgd050b15">神经网络的表示</a></li>
<li><a href="#orgeee3107">神经网络计算</a></li>
<li><a href="#orgea76611">多样本向量化</a></li>
<li><a href="#org28c1201">向量化实现的解释</a></li>
<li><a href="#org531c508">激活函数</a></li>
</ul>
</div>
</div>
<div id="outline-container-org773ead7" class="outline-2">
<h2 id="org773ead7">神经网络概述</h2>
<div class="outline-text-2" id="text-org773ead7">
<pre class="example">
    接下来将学习如何实现一个神经网络
</pre>

<p>
前面讨论了逻辑回归：
</p>


<div class="figure">
<p><img src="../pic/L1_week3_1.png" alt="L1_week3_1.png" width="50%" /> 
</p>
</div>

\begin{equation}                        
\left. 
\begin{array}{c}
x \\ 
w \\ 
b
\end{array}
\right\} \Longrightarrow z = w^Tx + b
\end{equation}                          

<p>
如上所示，首先需要输入特征 \(x\) ，参数 \(w\) 和 \(b\) ，通过这些就可以计算出 \(z\) : 
</p>

\begin{equation}                        
\left. 
\begin{array}{c}
x \\ 
w \\ 
b
\end{array}
\right\} \Longrightarrow z = w^Tx + b \Longrightarrow \alpha = \sigma(z) \Longrightarrow \mathbf{L}(a, y) 
\end{equation}                          

<p>
接下来使用 \(z\) 就可以计算出 \(\alpha\) 。把符号换为表示输出 \(\hat{y} \Longrightarrow \alpha = \sigma(z)\), 然后可以计算出损失函数 \(\mathbf{L}(a, y)\) 
</p>

<p>
神经网络如下图：把许多 \(\mathbf{sigmoid}\) 单元堆叠起来。对于图中的节点，它包含了之前讲的计算的两个步骤：首先计算出值 \(z\) ，然后通过 \(\sigma(z)\) 计算值 \(\alpha\) 
</p>



<div class="figure">
<p><img src="../pic/L1_week3_2.png" alt="L1_week3_2.png" width="50%" /> 
</p>
</div>

<p>
在这个神经网络对应的3个节点：首先计算第一层网络中的各个节点相关的数 \(z^{[i]}\) , 接着计算 \(\alpha^{[1]}\) ，在计算下一层网络会使用符号 \(^{[m]}\) 表示第 \(m\) 层网络中节点相关的数，这些节点的集合被称为第 \(m\) 层网络。这样可以保证不会和之前用来表示单个的训练样本的 \(^{(i)}\) (即使用表示第 \(i\) 个训练样本)混淆； 整个计算过程：
</p>

<p>
第一层：
</p>

\begin{equation}                        
\left. 
\begin{array}{c}
x \\ 
W^{[1]} \\ 
b^{[1]}
\end{array}
\right\} \Longrightarrow z^{[1]} = W^{[1]}x + b^{[1]} \Longrightarrow \alpha^{[1]} = \sigma(z^{[1]}) 
\end{equation}                          

<p>
第二层：
</p>

\begin{equation}                        
\left. 
\begin{array}{c}
\alpha^{[1]} = \sigma(z^{[1]}) \\ 
W^{[2]} \\ 
b^{[2]}
\end{array}
\right\} \Longrightarrow z^{[2]} = W^{[2]}\alpha^{[1]} + b^{[2]} \Longrightarrow \alpha^{[2]} = \sigma(z^{[2]}) \Longrightarrow \mathbf{L}(a^{[2]}, y)  
\end{equation}                          

<p>
使用另外一个线性方程对应的参数计算 \(z^{[2]}\) ，再计算\(\alpha^{[2]}\) ，这就是整个神经网络最终的输出，用\(\hat{y}\) 表示网络的输出 
</p>

<pre class="example">
    在逻辑回归中，通过直接计算得到结果

    而这个神经网络中，反复的计算 z 和 a，再计算 a 和 z，最后得到了最终的输出loss function
</pre>

<p>
逻辑回归里是从后向前的计算用来计算导数 \(\mathrm{d} \alpha\), \(\mathrm{d} z\) 。同样，在神经网络中也有从后向前的计算，通过计算 \(\mathrm{d} \alpha^{[2]}\), \(\mathrm{d} z^{[2]}\) ，然后计算 \(\mathrm{d} W^{[2]}\), \(\mathrm{d} b^{[2]}\) 等，从右到左反向计算：
</p>

\begin{equation}                        
\left. 
\begin{array}{c}
\mathrm{d} \alpha^{[1]} = \mathrm{d} \sigma(z^{[1]}) \\ 
\mathrm{d} W^{[2]} \\ 
\mathrm{d} b^{[2]}
\end{array}
\right\} \Longleftarrow \mathrm{d} z^{[2]} = \mathrm{d}(W^{[2]}\alpha^{[1]} + b^{[2]}) \Longleftarrow \mathrm{d} \alpha^{[2]} = \mathrm{d} \sigma(z^{[2]}) \Longleftarrow \mathrm{d} \mathbf{L}(a^{[2]}, y)  
\end{equation}                          

<pre class="example">
    接下来讲述神经网络的表示
</pre>
</div>
</div>
<div id="outline-container-orgd050b15" class="outline-2">
<h2 id="orgd050b15">神经网络的表示</h2>
<div class="outline-text-2" id="text-orgd050b15">
<p>
下面的神经网络只包含一个隐藏层：
</p>


<div class="figure">
<p><img src="../pic/L1_week3_3.png" alt="L1_week3_3.png" width="50%" /> 
</p>
</div>

<ul class="org-ul">
<li>首先有输入特征 \(x_1\) , \(x_2\) , \(x_3\) 它们被竖直地堆叠起来，这叫做神经网络的 <b>输入层</b> ：它包含了神经网络的输入</li>
<li><p>
接下来有另外一层四个结点称之为 <b>隐藏层</b> 
</p>
<pre class="example">
      在一个神经网络中，使用监督学习训练它的时候，训练集包含了输入也包含了目标输出

      隐藏层中的这些中间结点的准确值是不知道到的，也就是说看不见它们在训练集中应具有的值
</pre></li>
<li>最后一层只由一个结点构成，而这个只有一个结点的层被称为 <b>输出层</b> ，它负责产生预测值</li>
</ul>

<p>
现在再引入几个符号，就像之前用向量 \(x\) 表示输入特征。这里有个可代替的记号用来表示输入特征 \(a^{[0]}\) 。\(a\) 表示 <b>激活</b> 的意思，它意味着网络中不同层的值会传递到它们后面的层中：
</p>
<ul class="org-ul">
<li>输入层将 \(x\) 传递给隐藏层，所以将输入层的激活值称为 \(a^{[0]}\)</li>
<li>下一层即隐藏层也同样会产生一些激活值，那么将其记作 \(a^{[1]}\)</li>
</ul>

<p>
这里的第一个单元或结点将其表示为 \(a_1^{[1]}\) ，第二个结点的值记为 \(a_1^{[2]}\) 以此类推。所以这里的 \(a^{[1]}\) 是一个规模为 \(4 \times 1\) 的矩阵或一个大小为 <span class="underline">4</span> 的 <b>列向量</b> .z这里有四个结点或者单元，或者称为四个 <b>隐藏层单元</b> ： 
</p>

\begin{equation}                        
a^{[1]} = \begin{bmatrix} a_1^{[1]} \\ a_2^{[1]} \\ a_3^{[1]} \\ a_4^{[1]} \\ \end{bmatrix} 
\end{equation}                          

<p>
最后输出层将产生某个数值 \(a\) ，它只是一个单独的实数，所以的 \(\hat{y}\) 值将取为 \(a^{[2]}\) 。这与逻辑回归很相似，在逻辑回归中， \(\hat{y}\) 直接等于 \(a\) 
</p>

<pre class="example">
    在逻辑回归中只有一个输出层，所以没有用带方括号的上标

    但是在神经网络中，将使用这种带上标的形式来明确地指出这些值来自于哪一层

    有趣的是在约定俗成的符号传统中，这个例子只能叫做一个两层的神经网络
</pre>

<p>
原因是当计算网络的层数时，输入层是不算入总层数内，所以隐藏层是第一层，输出层是第二层。将输入层称为 <b>第零层</b> 
</p>


<div class="figure">
<p><img src="../pic/L1_week3_4.png" alt="L1_week3_4.png" width="50%" /> 
</p>
</div>

<p>
最后，要看到的隐藏层以及最后的输出层是带有参数的，这里的隐藏层将拥有两个参数 \(W\) 和 \(b\) , 将给它们加上上标\(^{[i]}\)( \(W^{[1]}\) , \(b^{[1]}\) ). 表示这些参数是和第一层这个隐藏层有关系的。 \(W^{[1]}\) 是一个\(4 \times 3\) 的矩阵，而 \(b^{[1]}\) 是一个\(4 \times 1\) 的向量
</p>

<pre class="example">
    第一个数字4源自于有四个结点或隐藏层单元，第二个数字3源自于这里有三个输入特征

    之后会更加详细地讨论这些矩阵的维数，到那时可能就更加清楚了
</pre>

<p>
相似的输出层也有一些与之关联的参数 \(W^{[2]}\) 以及 \(b^{[2]}\) 。从维数上来看，它们的规模分别是 \(1 \times 4\) 以及 \(1 \times 1\) . \(1 \times 4\) 是因为隐藏层有四个隐藏层单元而输出层只有一个单元 
</p>

<pre class="example">
    下面将更深入地了解这个神经网络是如何进行计算的
</pre>
</div>
</div>
<div id="outline-container-orgeee3107" class="outline-2">
<h2 id="orgeee3107">神经网络计算</h2>
<div class="outline-text-2" id="text-orgeee3107">
<p>
首先，回顾下只有一个隐藏层的简单两层神经网络结构，其中， \(x\) 表示输入特征，\(a\) 表示每个神经元的输出，\(W\) 表示特征的权重， <span class="underline">上标</span> 表示 <b>神经网络的层数</b> （隐藏层为1）， <span class="underline">下标</span> 表示 <b>该层的第几个神经元</b> 。这是神经网络的 <b>符号惯例</b>  
</p>


<div class="figure">
<p><img src="../pic/L1_week3_5.png" alt="L1_week3_5.png" width="50%" /> 
</p>
</div>


<p>
从逻辑回归开始，用 <span class="underline">圆圈</span> 表示 <b>神经网络的计算单元</b> ，逻辑回归的计算有两个步骤：
</p>
<ol class="org-ol">
<li>按步骤计算出 \(z\)</li>
<li>以 \(\mathbf{sigmoid}\) 函数为 <b>激活函数</b> 计算 \(a\)</li>
</ol>


<div class="figure">
<p><img src="../pic/L1_week3_6.png" alt="L1_week3_6.png" width="50%" /> 
</p>
</div>

<pre class="example">
    一个神经网络只是这样子做了好多次重复计算
</pre>

<p>
回到两层的神经网络，从隐藏层的第一个神经元开始计算，如上图第一个最上面的箭头所指。从上图可以看出，输入与逻辑回归相似，这个神经元的计算与逻辑回归一样分为两步， <b>小圆圈</b> 代表了计算的两个步骤：
</p>
<ol class="org-ol">
<li>计算 \(z_1^{[1]}\) : \(z_1^{[1]} = w_1^{[1]T}x + b_1^{[1]}\)</li>
<li>通过激活函数计算 \(a_1^{[1]}\) : \(a_1^{[1]} = \sigma(z_1^{[1]})\)</li>
</ol>

<p>
隐藏层的第二个以及后面两个神经元的计算过程一样，只是注意符号表示不同，最终分别得到 \(a_1^{[1]}\), \(a_2^{[1]}\), \(a_3^{[1]}\), \(a_4^{[1]}\) :
</p>

\begin{equation} 
    z_1^{[1]} = w_1^{[1]T}x + b_1^{[1]}, a_1^{[1]} = \sigma(z_1^{[1]}) \\ 
    z_2^{[1]} = w_2^{[1]T}x + b_2^{[1]}, a_2^{[1]} = \sigma(z_2^{[1]}) \\ 
    z_3^{[1]} = w_3^{[1]T}x + b_3^{[1]}, a_3^{[1]} = \sigma(z_3^{[1]}) \\ 
    z_4^{[1]} = w_4^{[1]T}x + b_4^{[1]}, a_4^{[1]} = \sigma(z_4^{[1]}) 
\end{equation} 

<pre class="example">
  接下来要做的就是把这四个等式向量化
</pre>
<p>
向量化的过程是将神经网络中的 <span class="underline">一层神经元参数</span> <b>纵向堆积</b> 起来，例如隐藏层中的 \(w\) 纵向堆积起来变成一个 \(4 \times 3\) 的矩阵，用符号 \(W^{[1]}\) 表示
</p>

<pre class="example">
    这里有四个逻辑回归单元，且每一个逻辑回归单元都有相对应的向量作为参数

    把这四个向量堆积在一起，就会得出这是一个 4×3 的矩阵
</pre>

<p>
因此：
</p>

\begin{equation}
   z^{[n]} = W^{[n]T}x + b^{[n]}
\end{equation}

<p>
和 
</p>
\begin{equation}
   a^{[n]} = \sigma(z^{[n]}) 
\end{equation}

<p>
详细过程：
</p>

\begin{equation}                        
   a^{[1]} = \begin{bmatrix} 
	      a_1^{[1]} \\ a_2^{[1]} \\ a_3^{[1]} \\ a_4^{[1]}
	     \end{bmatrix} = \sigma(z^{[1]}) 
\end{equation}                          


\begin{equation}
   \begin{bmatrix}
       z_1^{[1]} \\ z_2^{[1]} \\ z_3^{[1]} \\ z_4^{[1]}   
   \end{bmatrix}  = \overbrace{
		     \begin{bmatrix}
			 \ldots W_1^{[1]T} \ldots \\ \ldots W_2^{[1]T} \ldots \\ \ldots W_3^{[1]T} \ldots \\ \ldots W_4^{[1]T} \ldots 
		      \end{bmatrix}
			      }^{W^{[1]}} \cdot 
		    \overbrace{
		       \begin{bmatrix}
			  x_1 \\ x_2 \\ x_3   
		       \end{bmatrix}
			      }^{input} + 
		    \overbrace{
		       \begin{bmatrix}
			  b_1^{[1]} \\ b_2^{[1]} \\ b_3^{[1]} \\ b_4^{[1]}   
		    \end{bmatrix}
			      }^{b^[1]}
\end{equation}

<p>
对于神经网络的第一层，给予一个输入 \(x\) ，得到 \(a^{[1]}\) ，\(x\) 实际上可以表示为 \(a^{[0]}\) 。后一层的表示同样可以写成类似的形式，得到 \(a^{[2]}\) , \(\hat{y} = a^{[2]}\) 
</p>


<div class="figure">
<p><img src="../pic/L1_week3_7.png" alt="L1_week3_7.png" width="70%" /> 
</p>
</div>

<p>
上图中左半部分所示为神经网络，把网络左边部分盖住先忽略，那么最后的输出单元就相当于一个逻辑回归的计算单元。而有一个包含一层隐藏层的神经网络，需要去实现以计算得到输出的是右边的四个等式，并且可以看成是一个向量化的计算过程，计算出隐藏层的四个逻辑回归单元和整个隐藏层的输出结果，用编程实现需要的也只是这四行代码
</p>

<pre class="example">
    现在已经能够根据给出的一个单独的输入特征向量，运用代码计算出一个简单神经网络的输出。

    接下来将了解的是如何一次能够计算出不止一个样本的神经网络输出，而是能一次性计算整个训练集的输出
</pre>
</div>
</div>
<div id="outline-container-orgea76611" class="outline-2">
<h2 id="orgea76611">多样本向量化</h2>
<div class="outline-text-2" id="text-orgea76611">
<pre class="example">
  逻辑回归是将各个训练样本组合成矩阵，对矩阵的各列进行计算
</pre>

<p>
神经网络是通过对逻辑回归中的等式简单的变形，让神经网络计算出输出值。这种计算是所有的训练样本同时进行的，以下是实现它具体的步骤：
</p>


<div class="figure">
<p><img src="../pic/L1_week3_8.png" alt="L1_week3_8.png" width="50%" /> 
</p>
</div>

<p>
根据前面得到的四个等式可以计算出 \(z^{[1]}\) , \(a^{[1]}\) , \(z^{[2]}\) , \(a^{[2]}\) . 对于一个给定的输入特征向量 \(\mathbf{X}\) ，这四个等式可以计算出 \(a^{[2]}\) $ 等于 \(\hat{y}\) 。但这是针对于单一的训练样本，如果有 \(m\) 个训练样本,那么就需要重复这个过程：
</p>
<ol class="org-ol">
<li>用第一个训练样本 \(x^{(1)}\) 来计算出预测值 \(\hat{y}^{(1)} = a^{[2](1)}\) ，就是第一个训练样本上得出的结果
<ul class="org-ul">
<li>\(a^{[2]{(1)}}\) : 这里的 \(^{(1)}\) 是指第 \(1\) 个训练样本，而 \(^{[2]}\) 是指第二层</li>
</ul></li>
<li>用第二个训练样本 \(x^{(2)}\) 来计算出预测值 \(\hat{y}^{(2)} = a^{[2](2)}\)</li>
<li>循环往复，直至用 \(x^{(m)}\) 计算出 \(\hat{y}^{(m)} = a^{[2](m)}\)</li>
</ol>

<p>
如果有一个非向量化形式的实现，而且要计算出它的预测值，对于所有训练样本，需要让 \(i\) 从 \(1\) 到 \(m\) 实现这四个等式：
</p>

\begin{equation}
   z^{[1](i)} = W^{[1](i)}x^{(i)} + b^{[1](i)} \\ 
   a^{[1](i)} = \sigma(z^{[1](i)}) \\  
   z^{[2](i)} = W^{[2](i)}x^{(i)} + b^{[2](i)} \\ 
   a^{[2](i)} = \sigma(z^{[2](i)}) 
\end{equation}

<p>
对于上面的这个方程中的 \(^{(i)}\) ，是所有依赖于训练样本的变量，即将 \((i)\) 添加到 \(x\) ，\(z\) 和 \(a\) 。如果想计算个训练样本上的所有输出，就应该向量化整个计算：
</p>

\begin{equation}
    X = \begin{bmatrix}
             \vdots & \vdots & \vdots & \vdots \\
             x_{(1)} & x_{(2)} & \cdots & x_{(m)} \\
             \vdots & \vdots & \vdots  & \vdots \\
       \end{bmatrix}
\end{equation}

<p>
定义矩阵 \(mathbf{X}\) 等于训练样本，将它们组合成矩阵的各列，形成一个 \(n \times m\) 维矩阵
</p>

<pre class="example">
以此类推，从小写的向量x到这个大写的矩阵X，只是通过组合向量在矩阵的各列中
</pre>

\begin{equation}
    Z^{[1]} = \begin{bmatrix}
             \vdots & \vdots & \vdots & \vdots \\
             z_{[1](1)} & z_{[1](2)} & \cdots & z_{[1](m)} \\
             \vdots & \vdots & \vdots  & \vdots \\
       \end{bmatrix}
\end{equation}

<p>
\(z_{[1](1)} & z_{[1](2)} & \cdots & z_{[1](m)}\) 等都是列向量，将所有 \(m\) 都组合在各列中，就的到矩阵 \(Z^{[1]}\) 
</p>

\begin{equation}
    A^{[1]} = \begin{bmatrix}
             \vdots & \vdots & \vdots & \vdots \\
             a_{[1](1)} & a_{[1](2)} & \cdots & a_{[1](m)} \\
             \vdots & \vdots & \vdots  & \vdots \\
       \end{bmatrix}
\end{equation}

<p>
\(a_{[1](1)} & a_{[1](2)} & \cdots & a_{[1](m)}\) 将其组合在矩阵各列中，就能得到矩阵 \(A^{[1]}\)
</p>

\begin{equation}                        
\left.
   \begin{array}{c}
   z^{[1](i)} = W^{[1](i)}x^{(i)} + b^{[1](i)} \\ 
   a^{[1](i)} = \sigma(z^{[1](i)}) \\  
   z^{[2](i)} = W^{[2](i)}x^{(i)} + b^{[2](i)} \\ 
   a^{[2](i)} = \sigma(z^{[2](i)})
   \end{array}  
\right\} \Longrightarrow  
\left\{
\begin{array}{c}
A^{[1]} = \sigma(Z^{[1]}) \\ 
Z^{[2]} = W^{[2]}A^{[1]}+ b^{[2]} \\ 
A^{[2]} = \sigma(Z^{[2]})
\end{array}  
\right. 
\end{equation}     

<p>
同样的，对于和 \(Z^{[2]}\) , \(A^{[2]}\) 也是这样得到 
</p>

<p>
这种符号其中一个作用就是，可以通过训练样本来进行索引：
</p>
<ul class="org-ul">
<li>水平索引对应于不同的训练样本的原因，这些训练样本是从左到右扫描训练集而得到的</li>
<li>在垂直方向，这个垂直索引对应于神经网络中的不同节点</li>
</ul>

<pre class="example">
  从水平上看，矩阵A代表了各个训练样本。从竖直上看，矩阵的不同的索引对应于不同的隐藏单元

  对于矩阵Z，X 情况也类似，水平方向上，对应于不同的训练样本；竖直方向上，对应不同的输入特征，而这就是神经网络输入层中各个节点
</pre>


<p>
神经网络上通过在多样本情况下的向量化来使用这些等式
</p>
</div>
</div>
<div id="outline-container-org28c1201" class="outline-2">
<h2 id="org28c1201">向量化实现的解释</h2>
<div class="outline-text-2" id="text-org28c1201">
<p>
先手动对几个样本计算一下前向传播：
</p>

\begin{equation}
   z^{[1](1)} = W^{[1](1)}x^{(1)} + b^{[1](1)} \\  
   z^{[1](2)} = W^{[1](2)}x^{(2)} + b^{[1](2)} \\ 
   z^{[1](3)} = W^{[1](3)}x^{(3)} + b^{[1](3)} \\ 
\end{equation}

<pre class="example">
这里，为了描述的简便，先忽略掉b^[1] 后面将会看到利用Python 的广播机制，可以很容易的将它加进来 
</pre>

<p>
现在 \(W^{[1]}\) 是一个矩阵，\(x^{(1)}\) , \(x^{(2)}\) , \(x^{(3)}\) 都是列向量，矩阵乘以列向量得到列向量，下面更直观的表示出来: 
</p>

\begin{equation}
W^{[1]}x = \begin{bmatrix}
            \ldots \\ 
            \ldots \\ 
            \ldots  
           \end{bmatrix} \cdot 
           \begin{bmatrix}
            \vdots & \vdots & \vdots & \vdots\\
            x^{(1)}& x^{(2)} & x^{(3)} & \vdots \\
            \vdots & \vdots & \vdots & \vdots 
           \end{bmatrix} = 
            \begin{bmatrix}
            \vdots & \vdots & \vdots & \vdots\\
            w^{[1]}x^{(1)}& w^{[1]}x^{(2)} & w^{[1]}x^{(3)} & \vdots \\
            \vdots & \vdots & \vdots & \vdots
           \end{bmatrix} =
            \begin{bmatrix}
            \vdots & \vdots & \vdots & \vdots\\
            z^{[1](1)}& z^{[1](2)} & z^{[1](3)} & \vdots \\
            \vdots & \vdots & \vdots & \vdots
           \end{bmatrix} = Z^{[1]}
\end{equation} 

<p>
这里也可以了解到，为什么之前对单个样本的计算要写成 \(z^{[1](1)} = W^{[1](1)}x^{(1)} + b^{[1](1)}\) 这种形式，因为当有不同的训练样本时，将它们堆到矩阵 \(X\) 的各列中，那么它们的输出也就会相应的堆叠到矩阵 \(Z^{[1]}\) 的各列中。现在就可以直接计算矩阵\(Z^{[1]}\) 加上 \(b^{[1]}\) ，因为列向量 \(b^{[1]}\) 和矩阵 \(Z^{[1]}\) 的列向量有着相同的尺寸，而Python的广播机制对于这种矩阵与向量直接相加的处理方式是，将向量与矩阵的每一列相加
</p>

<pre class="example">
  这里只是说明了前向传播的第一步计算的正确向量化实现

  但类似的分析可以发现，前向传播的其它步也可以使用非常相似的逻辑

  即如果将输入按列向量横向堆叠进矩阵，那么通过公式计算之后，也能得到成列堆叠的输出
</pre>

<p>
总结：使用向量化的方法，可以不需要显示循环，而直接通过矩阵运算从 \(X\) 就可以计算出 \(A^{[1]}\) ，实际上 \(X\) 可以记为 \(A^{[0]}\) ，使用同样的方法就可以由神经网络中的每一层的输入 \(A^{[i-1]}\) 计算输出 \(A^{[i]}\) 。这些方程有一定对称性，实际上第一个方程也可以写成 \(Z^{[1]} = W^{[1]}A^{[0]} + b^{[1]}\) 
</p>

<pre class="example">
  以上就是对神经网络向量化实现的正确性的解释

  到目前为止，仅使用sigmoid函数作为激活函数，事实上这并非最好的选择
</pre>
</div>
</div>
<div id="outline-container-org531c508" class="outline-2">
<h2 id="org531c508">激活函数</h2>
</div>
</div>
<div id="postamble" class="status">

		  <br/>
		  <div class='ds-thread'></div>
		  <script>
		  var duoshuoQuery = {short_name:'klose911'};
		  (function() {
					  var dsThread = document.getElementsByClassName('ds-thread')[0];
					  dsThread.setAttribute('data-thread-key', document.title);
					  dsThread.setAttribute('data-title', document.title);
					  dsThread.setAttribute('data-url', window.location.href);
					  var ds = document.createElement('script');
					  ds.type = 'text/javascript';ds.async = true;
					  ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
					  ds.charset = 'UTF-8';
					  (document.getElementsByTagName('head')[0] 
						|| document.getElementsByTagName('body')[0]).appendChild(ds);
					  })();
		  </script>
		  <script>
		  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
			(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
			m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
			})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
		  ga('create', 'UA-90850421-1', 'auto');
		  ga('send', 'pageview');
		  </script>
</div>
</body>
</html>
