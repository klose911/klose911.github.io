<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>浅层神经网络</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Wu, Shanliang" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="../css/main.css" />
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2019 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="./basics.html"> UP </a>
 |
 <a accesskey="H" href="./neural-network.html"> HOME </a>
</div><div id="content">
<h1 class="title">浅层神经网络</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org773ead7">神经网络概述</a></li>
<li><a href="#orgd050b15">神经网络的表示</a></li>
<li><a href="#orgeee3107">神经网络计算</a></li>
<li><a href="#orgea76611">多样本向量化</a></li>
<li><a href="#org28c1201">向量化实现的解释</a></li>
<li><a href="#org531c508">激活函数</a>
<ul>
<li><a href="#org3414fc0">总结</a></li>
</ul>
</li>
<li><a href="#orgd2886c4">为什么需要非线性激活函数?</a></li>
<li><a href="#org0410b46">激活函数的导数</a>
<ul>
<li><a href="#org47377f3">sigmod</a></li>
<li><a href="#org6cff32c">tanh</a></li>
<li><a href="#orgbeefd36">Relu</a></li>
<li><a href="#org9c45b6c">Leaky Relu</a></li>
</ul>
</li>
<li><a href="#orgc436ab0">神经网络的梯度下降</a></li>
</ul>
</div>
</div>
<div id="outline-container-org773ead7" class="outline-2">
<h2 id="org773ead7">神经网络概述</h2>
<div class="outline-text-2" id="text-org773ead7">
<pre class="example">
    接下来将学习如何实现一个神经网络
</pre>

<p>
前面讨论了逻辑回归：
</p>


<div class="figure">
<p><img src="../pic/L1_week3_1.png" alt="L1_week3_1.png" width="50%" /> 
</p>
</div>

\begin{equation}                        
\left. 
\begin{array}{c}
x \\ 
w \\ 
b
\end{array}
\right\} \Longrightarrow z = w^Tx + b
\end{equation}                          

<p>
如上所示，首先需要输入特征 \(x\) ，参数 \(w\) 和 \(b\) ，通过这些就可以计算出 \(z\) : 
</p>

\begin{equation}                        
\left. 
\begin{array}{c}
x \\ 
w \\ 
b
\end{array}
\right\} \Longrightarrow z = w^Tx + b \Longrightarrow \alpha = \sigma(z) \Longrightarrow \mathbf{L}(a, y) 
\end{equation}                          

<p>
接下来使用 \(z\) 就可以计算出 \(\alpha\) 。把符号换为表示输出 \(\hat{y} \Longrightarrow \alpha = \sigma(z)\), 然后可以计算出损失函数 \(\mathbf{L}(a, y)\) 
</p>

<p>
神经网络如下图：把许多 \(\mathbf{sigmoid}\) 单元堆叠起来。对于图中的节点，它包含了之前讲的计算的两个步骤：首先计算出值 \(z\) ，然后通过 \(\sigma(z)\) 计算值 \(\alpha\) 
</p>



<div class="figure">
<p><img src="../pic/L1_week3_2.png" alt="L1_week3_2.png" width="50%" /> 
</p>
</div>

<p>
在这个神经网络对应的3个节点：首先计算第一层网络中的各个节点相关的数 \(z^{[i]}\) , 接着计算 \(\alpha^{[1]}\) ，在计算下一层网络会使用符号 \(^{[m]}\) 表示第 \(m\) 层网络中节点相关的数，这些节点的集合被称为第 \(m\) 层网络。这样可以保证不会和之前用来表示单个的训练样本的 \(^{(i)}\) (即使用表示第 \(i\) 个训练样本)混淆； 整个计算过程：
</p>

<p>
第一层：
</p>

\begin{equation}                        
\left. 
\begin{array}{c}
x \\ 
W^{[1]} \\ 
b^{[1]}
\end{array}
\right\} \Longrightarrow z^{[1]} = W^{[1]}x + b^{[1]} \Longrightarrow \alpha^{[1]} = \sigma(z^{[1]}) 
\end{equation}                          

<p>
第二层：
</p>

\begin{equation}                        
\left. 
\begin{array}{c}
\alpha^{[1]} = \sigma(z^{[1]}) \\ 
W^{[2]} \\ 
b^{[2]}
\end{array}
\right\} \Longrightarrow z^{[2]} = W^{[2]}\alpha^{[1]} + b^{[2]} \Longrightarrow \alpha^{[2]} = \sigma(z^{[2]}) \Longrightarrow \mathbf{L}(a^{[2]}, y)  
\end{equation}                          

<p>
使用另外一个线性方程对应的参数计算 \(z^{[2]}\) ，再计算\(\alpha^{[2]}\) ，这就是整个神经网络最终的输出，用\(\hat{y}\) 表示网络的输出 
</p>

<pre class="example">
    在逻辑回归中，通过直接计算得到结果

    而这个神经网络中，反复的计算 z 和 a，再计算 a 和 z，最后得到了最终的输出loss function
</pre>

<p>
逻辑回归里是从后向前的计算用来计算导数 \(\mathrm{d} \alpha\), \(\mathrm{d} z\) 。同样，在神经网络中也有从后向前的计算，通过计算 \(\mathrm{d} \alpha^{[2]}\), \(\mathrm{d} z^{[2]}\) ，然后计算 \(\mathrm{d} W^{[2]}\), \(\mathrm{d} b^{[2]}\) 等，从右到左反向计算：
</p>

\begin{equation}                        
\left. 
\begin{array}{c}
\mathrm{d} \alpha^{[1]} = \mathrm{d} \sigma(z^{[1]}) \\ 
\mathrm{d} W^{[2]} \\ 
\mathrm{d} b^{[2]}
\end{array}
\right\} \Longleftarrow \mathrm{d} z^{[2]} = \mathrm{d}(W^{[2]}\alpha^{[1]} + b^{[2]}) \Longleftarrow \mathrm{d} \alpha^{[2]} = \mathrm{d} \sigma(z^{[2]}) \Longleftarrow \mathrm{d} \mathbf{L}(a^{[2]}, y)  
\end{equation}                          

<pre class="example">
    接下来讲述神经网络的表示
</pre>
</div>
</div>
<div id="outline-container-orgd050b15" class="outline-2">
<h2 id="orgd050b15">神经网络的表示</h2>
<div class="outline-text-2" id="text-orgd050b15">
<p>
下面的神经网络只包含一个隐藏层：
</p>


<div class="figure">
<p><img src="../pic/L1_week3_3.png" alt="L1_week3_3.png" width="50%" /> 
</p>
</div>

<ul class="org-ul">
<li>首先有输入特征 \(x_1\) , \(x_2\) , \(x_3\) 它们被竖直地堆叠起来，这叫做神经网络的 <b>输入层</b> ：它包含了神经网络的输入</li>
<li><p>
接下来有另外一层四个结点称之为 <b>隐藏层</b> 
</p>
<pre class="example">
      在一个神经网络中，使用监督学习训练它的时候，训练集包含了输入也包含了目标输出

      隐藏层中的这些中间结点的准确值是不知道到的，也就是说看不见它们在训练集中应具有的值
</pre></li>
<li>最后一层只由一个结点构成，而这个只有一个结点的层被称为 <b>输出层</b> ，它负责产生预测值</li>
</ul>

<p>
现在再引入几个符号，就像之前用向量 \(x\) 表示输入特征。这里有个可代替的记号用来表示输入特征 \(a^{[0]}\) 。\(a\) 表示 <b>激活</b> 的意思，它意味着网络中不同层的值会传递到它们后面的层中：
</p>
<ul class="org-ul">
<li>输入层将 \(x\) 传递给隐藏层，所以将输入层的激活值称为 \(a^{[0]}\)</li>
<li>下一层即隐藏层也同样会产生一些激活值，那么将其记作 \(a^{[1]}\)</li>
</ul>

<p>
这里的第一个单元或结点将其表示为 \(a_1^{[1]}\) ，第二个结点的值记为 \(a_1^{[2]}\) 以此类推。所以这里的 \(a^{[1]}\) 是一个规模为 \(4 \times 1\) 的矩阵或一个大小为 <span class="underline">4</span> 的 <b>列向量</b> .z这里有四个结点或者单元，或者称为四个 <b>隐藏层单元</b> ： 
</p>

\begin{equation}                        
a^{[1]} = \begin{bmatrix} a_1^{[1]} \\ a_2^{[1]} \\ a_3^{[1]} \\ a_4^{[1]} \\ \end{bmatrix} 
\end{equation}                          

<p>
最后输出层将产生某个数值 \(a\) ，它只是一个单独的实数，所以的 \(\hat{y}\) 值将取为 \(a^{[2]}\) 。这与逻辑回归很相似，在逻辑回归中， \(\hat{y}\) 直接等于 \(a\) 
</p>

<pre class="example">
    在逻辑回归中只有一个输出层，所以没有用带方括号的上标

    但是在神经网络中，将使用这种带上标的形式来明确地指出这些值来自于哪一层

    有趣的是在约定俗成的符号传统中，这个例子只能叫做一个两层的神经网络
</pre>

<p>
原因是当计算网络的层数时，输入层是不算入总层数内，所以隐藏层是第一层，输出层是第二层。将输入层称为 <b>第零层</b> 
</p>


<div class="figure">
<p><img src="../pic/L1_week3_4.png" alt="L1_week3_4.png" width="50%" /> 
</p>
</div>

<p>
最后，要看到的隐藏层以及最后的输出层是带有参数的，这里的隐藏层将拥有两个参数 \(W\) 和 \(b\) , 将给它们加上上标\(^{[i]}\)( \(W^{[1]}\) , \(b^{[1]}\) ). 表示这些参数是和第一层这个隐藏层有关系的。 \(W^{[1]}\) 是一个\(4 \times 3\) 的矩阵，而 \(b^{[1]}\) 是一个\(4 \times 1\) 的向量
</p>

<pre class="example">
    第一个数字4源自于有四个结点或隐藏层单元，第二个数字3源自于这里有三个输入特征

    之后会更加详细地讨论这些矩阵的维数，到那时可能就更加清楚了
</pre>

<p>
相似的输出层也有一些与之关联的参数 \(W^{[2]}\) 以及 \(b^{[2]}\) 。从维数上来看，它们的规模分别是 \(1 \times 4\) 以及 \(1 \times 1\) . \(1 \times 4\) 是因为隐藏层有四个隐藏层单元而输出层只有一个单元 
</p>

<pre class="example">
    下面将更深入地了解这个神经网络是如何进行计算的
</pre>
</div>
</div>
<div id="outline-container-orgeee3107" class="outline-2">
<h2 id="orgeee3107">神经网络计算</h2>
<div class="outline-text-2" id="text-orgeee3107">
<p>
首先，回顾下只有一个隐藏层的简单两层神经网络结构，其中， \(x\) 表示输入特征，\(a\) 表示每个神经元的输出，\(W\) 表示特征的权重， <span class="underline">上标</span> 表示 <b>神经网络的层数</b> （隐藏层为1）， <span class="underline">下标</span> 表示 <b>该层的第几个神经元</b> 。这是神经网络的 <b>符号惯例</b>  
</p>


<div class="figure">
<p><img src="../pic/L1_week3_5.png" alt="L1_week3_5.png" width="50%" /> 
</p>
</div>


<p>
从逻辑回归开始，用 <span class="underline">圆圈</span> 表示 <b>神经网络的计算单元</b> ，逻辑回归的计算有两个步骤：
</p>
<ol class="org-ol">
<li>按步骤计算出 \(z\)</li>
<li>以 \(\mathbf{sigmoid}\) 函数为 <b>激活函数</b> 计算 \(a\)</li>
</ol>


<div class="figure">
<p><img src="../pic/L1_week3_6.png" alt="L1_week3_6.png" width="50%" /> 
</p>
</div>

<pre class="example">
    一个神经网络只是这样子做了好多次重复计算
</pre>

<p>
回到两层的神经网络，从隐藏层的第一个神经元开始计算，如上图第一个最上面的箭头所指。从上图可以看出，输入与逻辑回归相似，这个神经元的计算与逻辑回归一样分为两步， <b>小圆圈</b> 代表了计算的两个步骤：
</p>
<ol class="org-ol">
<li>计算 \(z_1^{[1]}\) : \(z_1^{[1]} = w_1^{[1]T}x + b_1^{[1]}\)</li>
<li>通过激活函数计算 \(a_1^{[1]}\) : \(a_1^{[1]} = \sigma(z_1^{[1]})\)</li>
</ol>

<p>
隐藏层的第二个以及后面两个神经元的计算过程一样，只是注意符号表示不同，最终分别得到 \(a_1^{[1]}\), \(a_2^{[1]}\), \(a_3^{[1]}\), \(a_4^{[1]}\) :
</p>

\begin{equation} 
    z_1^{[1]} = w_1^{[1]T}x + b_1^{[1]}, a_1^{[1]} = \sigma(z_1^{[1]}) \\ 
    z_2^{[1]} = w_2^{[1]T}x + b_2^{[1]}, a_2^{[1]} = \sigma(z_2^{[1]}) \\ 
    z_3^{[1]} = w_3^{[1]T}x + b_3^{[1]}, a_3^{[1]} = \sigma(z_3^{[1]}) \\ 
    z_4^{[1]} = w_4^{[1]T}x + b_4^{[1]}, a_4^{[1]} = \sigma(z_4^{[1]}) 
\end{equation} 

<pre class="example">
  接下来要做的就是把这四个等式向量化
</pre>
<p>
向量化的过程是将神经网络中的 <span class="underline">一层神经元参数</span> <b>纵向堆积</b> 起来，例如隐藏层中的 \(w\) 纵向堆积起来变成一个 \(4 \times 3\) 的矩阵，用符号 \(W^{[1]}\) 表示
</p>

<pre class="example">
    这里有四个逻辑回归单元，且每一个逻辑回归单元都有相对应的向量作为参数

    把这四个向量堆积在一起，就会得出这是一个 4×3 的矩阵
</pre>

<p>
因此：
</p>

\begin{equation}
   z^{[n]} = W^{[n]T}x + b^{[n]}
\end{equation}

<p>
和 
</p>
\begin{equation}
   a^{[n]} = \sigma(z^{[n]}) 
\end{equation}

<p>
详细过程：
</p>

\begin{equation}                        
   a^{[1]} = \begin{bmatrix} 
	      a_1^{[1]} \\ a_2^{[1]} \\ a_3^{[1]} \\ a_4^{[1]}
	     \end{bmatrix} = \sigma(z^{[1]}) 
\end{equation}                          


\begin{equation}
   \begin{bmatrix}
       z_1^{[1]} \\ z_2^{[1]} \\ z_3^{[1]} \\ z_4^{[1]}   
   \end{bmatrix}  = \overbrace{
		     \begin{bmatrix}
			 \ldots W_1^{[1]T} \ldots \\ \ldots W_2^{[1]T} \ldots \\ \ldots W_3^{[1]T} \ldots \\ \ldots W_4^{[1]T} \ldots 
		      \end{bmatrix}
			      }^{W^{[1]}} \cdot 
		    \overbrace{
		       \begin{bmatrix}
			  x_1 \\ x_2 \\ x_3   
		       \end{bmatrix}
			      }^{input} + 
		    \overbrace{
		       \begin{bmatrix}
			  b_1^{[1]} \\ b_2^{[1]} \\ b_3^{[1]} \\ b_4^{[1]}   
		    \end{bmatrix}
			      }^{b^[1]}
\end{equation}

<p>
对于神经网络的第一层，给予一个输入 \(x\) ，得到 \(a^{[1]}\) ，\(x\) 实际上可以表示为 \(a^{[0]}\) 。后一层的表示同样可以写成类似的形式，得到 \(a^{[2]}\) , \(\hat{y} = a^{[2]}\) 
</p>


<div class="figure">
<p><img src="../pic/L1_week3_7.png" alt="L1_week3_7.png" width="70%" /> 
</p>
</div>

<p>
上图中左半部分所示为神经网络，把网络左边部分盖住先忽略，那么最后的输出单元就相当于一个逻辑回归的计算单元。而有一个包含一层隐藏层的神经网络，需要去实现以计算得到输出的是右边的四个等式，并且可以看成是一个向量化的计算过程，计算出隐藏层的四个逻辑回归单元和整个隐藏层的输出结果，用编程实现需要的也只是这四行代码
</p>

<pre class="example">
    现在已经能够根据给出的一个单独的输入特征向量，运用代码计算出一个简单神经网络的输出。

    接下来将了解的是如何一次能够计算出不止一个样本的神经网络输出，而是能一次性计算整个训练集的输出
</pre>
</div>
</div>
<div id="outline-container-orgea76611" class="outline-2">
<h2 id="orgea76611">多样本向量化</h2>
<div class="outline-text-2" id="text-orgea76611">
<pre class="example">
    逻辑回归是将各个训练样本组合成矩阵，对矩阵的各列进行计算
</pre>

<p>
神经网络是通过对逻辑回归中的等式简单的变形，让神经网络计算出输出值。这种计算是所有的训练样本同时进行的，以下是实现它具体的步骤：
</p>


<div class="figure">
<p><img src="../pic/L1_week3_8.png" alt="L1_week3_8.png" width="50%" /> 
</p>
</div>

<p>
根据前面得到的四个等式可以计算出 \(z^{[1]}\) , \(a^{[1]}\) , \(z^{[2]}\) , \(a^{[2]}\) . 对于一个给定的输入特征向量 \(\mathbf{X}\) ，这四个等式可以计算出 \(a^{[2]}\) $ 等于 \(\hat{y}\) 。但这是针对于单一的训练样本，如果有 \(m\) 个训练样本,那么就需要重复这个过程：
</p>
<ol class="org-ol">
<li>用第一个训练样本 \(x^{(1)}\) 来计算出预测值 \(\hat{y}^{(1)} = a^{[2](1)}\) ，就是第一个训练样本上得出的结果
<ul class="org-ul">
<li>\(a^{[2]{(1)}}\) : 这里的 \(^{(1)}\) 是指第 \(1\) 个训练样本，而 \(^{[2]}\) 是指第二层</li>
</ul></li>
<li>用第二个训练样本 \(x^{(2)}\) 来计算出预测值 \(\hat{y}^{(2)} = a^{[2](2)}\)</li>
<li>循环往复，直至用 \(x^{(m)}\) 计算出 \(\hat{y}^{(m)} = a^{[2](m)}\)</li>
</ol>

<p>
如果有一个非向量化形式的实现，而且要计算出它的预测值，对于所有训练样本，需要让 \(i\) 从 \(1\) 到 \(m\) 实现这四个等式：
</p>

\begin{equation}
   z^{[1](i)} = W^{[1](i)}x^{(i)} + b^{[1](i)} \\ 
   a^{[1](i)} = \sigma(z^{[1](i)}) \\  
   z^{[2](i)} = W^{[2](i)}x^{(i)} + b^{[2](i)} \\ 
   a^{[2](i)} = \sigma(z^{[2](i)}) 
\end{equation}

<p>
对于上面的这个方程中的 \(^{(i)}\) ，是所有依赖于训练样本的变量，即将 \((i)\) 添加到 \(x\) ，\(z\) 和 \(a\) 。如果想计算个训练样本上的所有输出，就应该向量化整个计算：
</p>

\begin{equation}
    X = \begin{bmatrix}
	     \vdots & \vdots & \vdots & \vdots \\
	     x_{(1)} & x_{(2)} & \cdots & x_{(m)} \\
	     \vdots & \vdots & \vdots  & \vdots \\
       \end{bmatrix}
\end{equation}

<p>
定义矩阵 \(mathbf{X}\) 等于训练样本，将它们组合成矩阵的各列，形成一个 \(n \times m\) 维矩阵
</p>

<pre class="example">
  以此类推，从小写的向量x到这个大写的矩阵X，只是通过组合向量在矩阵的各列中
</pre>

\begin{equation}
    Z^{[1]} = \begin{bmatrix}
	     \vdots & \vdots & \vdots & \vdots \\
	     z_{[1](1)} & z_{[1](2)} & \cdots & z_{[1](m)} \\
	     \vdots & \vdots & \vdots  & \vdots \\
       \end{bmatrix}
\end{equation}

<p>
\(z_{[1](1)} & z_{[1](2)} & \cdots & z_{[1](m)}\) 等都是列向量，将所有 \(m\) 都组合在各列中，就的到矩阵 \(Z^{[1]}\) 
</p>

\begin{equation}
    A^{[1]} = \begin{bmatrix}
	     \vdots & \vdots & \vdots & \vdots \\
	     a_{[1](1)} & a_{[1](2)} & \cdots & a_{[1](m)} \\
	     \vdots & \vdots & \vdots  & \vdots \\
       \end{bmatrix}
\end{equation}

<p>
\(a_{[1](1)} & a_{[1](2)} & \cdots & a_{[1](m)}\) 将其组合在矩阵各列中，就能得到矩阵 \(A^{[1]}\)
</p>

\begin{equation}                        
\left.
   \begin{array}{c}
   z^{[1](i)} = W^{[1](i)}x^{(i)} + b^{[1](i)} \\ 
   a^{[1](i)} = \sigma(z^{[1](i)}) \\  
   z^{[2](i)} = W^{[2](i)}x^{(i)} + b^{[2](i)} \\ 
   a^{[2](i)} = \sigma(z^{[2](i)})
   \end{array}  
\right\} \Longrightarrow  
\left\{
\begin{array}{c}
A^{[1]} = \sigma(Z^{[1]}) \\ 
Z^{[2]} = W^{[2]}A^{[1]}+ b^{[2]} \\ 
A^{[2]} = \sigma(Z^{[2]})
\end{array}  
\right. 
\end{equation}     

<p>
同样的，对于和 \(Z^{[2]}\) , \(A^{[2]}\) 也是这样得到 
</p>

<p>
这种符号其中一个作用就是，可以通过训练样本来进行索引：
</p>
<ul class="org-ul">
<li>水平索引对应于不同的训练样本的原因，这些训练样本是从左到右扫描训练集而得到的</li>
<li>在垂直方向，这个垂直索引对应于神经网络中的不同节点</li>
</ul>

<pre class="example">
    从水平上看，矩阵A代表了各个训练样本。从竖直上看，矩阵的不同的索引对应于不同的隐藏单元

    对于矩阵Z，X 情况也类似，水平方向上，对应于不同的训练样本；竖直方向上，对应不同的输入特征，而这就是神经网络输入层中各个节点
</pre>


<p>
神经网络上通过在多样本情况下的向量化来使用这些等式
</p>
</div>
</div>
<div id="outline-container-org28c1201" class="outline-2">
<h2 id="org28c1201">向量化实现的解释</h2>
<div class="outline-text-2" id="text-org28c1201">
<p>
先手动对几个样本计算一下前向传播：
</p>

\begin{equation}
   z^{[1](1)} = W^{[1](1)}x^{(1)} + b^{[1](1)} \\  
   z^{[1](2)} = W^{[1](2)}x^{(2)} + b^{[1](2)} \\ 
   z^{[1](3)} = W^{[1](3)}x^{(3)} + b^{[1](3)} \\ 
\end{equation}

<pre class="example">
  这里，为了描述的简便，先忽略掉b^[1] 后面将会看到利用Python 的广播机制，可以很容易的将它加进来 
</pre>

<p>
现在 \(W^{[1]}\) 是一个矩阵，\(x^{(1)}\) , \(x^{(2)}\) , \(x^{(3)}\) 都是列向量，矩阵乘以列向量得到列向量，下面更直观的表示出来: 
</p>

\begin{equation}
W^{[1]}x = \begin{bmatrix}
	    \ldots \\ 
	    \ldots \\ 
	    \ldots  
	   \end{bmatrix} \cdot 
	   \begin{bmatrix}
	    \vdots & \vdots & \vdots & \vdots\\
	    x^{(1)}& x^{(2)} & x^{(3)} & \vdots \\
	    \vdots & \vdots & \vdots & \vdots 
	   \end{bmatrix} = 
	    \begin{bmatrix}
	    \vdots & \vdots & \vdots & \vdots\\
	    w^{[1]}x^{(1)}& w^{[1]}x^{(2)} & w^{[1]}x^{(3)} & \vdots \\
	    \vdots & \vdots & \vdots & \vdots
	   \end{bmatrix} =
	    \begin{bmatrix}
	    \vdots & \vdots & \vdots & \vdots\\
	    z^{[1](1)}& z^{[1](2)} & z^{[1](3)} & \vdots \\
	    \vdots & \vdots & \vdots & \vdots
	   \end{bmatrix} = Z^{[1]}
\end{equation} 

<p>
这里也可以了解到，为什么之前对单个样本的计算要写成 \(z^{[1](1)} = W^{[1](1)}x^{(1)} + b^{[1](1)}\) 这种形式，因为当有不同的训练样本时，将它们堆到矩阵 \(X\) 的各列中，那么它们的输出也就会相应的堆叠到矩阵 \(Z^{[1]}\) 的各列中。现在就可以直接计算矩阵\(Z^{[1]}\) 加上 \(b^{[1]}\) ，因为列向量 \(b^{[1]}\) 和矩阵 \(Z^{[1]}\) 的列向量有着相同的尺寸，而Python的广播机制对于这种矩阵与向量直接相加的处理方式是，将向量与矩阵的每一列相加
</p>

<pre class="example">
    这里只是说明了前向传播的第一步计算的正确向量化实现

    但类似的分析可以发现，前向传播的其它步也可以使用非常相似的逻辑

    即如果将输入按列向量横向堆叠进矩阵，那么通过公式计算之后，也能得到成列堆叠的输出
</pre>

<p>
总结：使用向量化的方法，可以不需要显示循环，而直接通过矩阵运算从 \(X\) 就可以计算出 \(A^{[1]}\) ，实际上 \(X\) 可以记为 \(A^{[0]}\) ，使用同样的方法就可以由神经网络中的每一层的输入 \(A^{[i-1]}\) 计算输出 \(A^{[i]}\) 。这些方程有一定对称性，实际上第一个方程也可以写成 \(Z^{[1]} = W^{[1]}A^{[0]} + b^{[1]}\) 
</p>

<pre class="example">
    以上就是对神经网络向量化实现的正确性的解释

    到目前为止，仅使用sigmoid函数作为激活函数，事实上这并非最好的选择
</pre>
</div>
</div>
<div id="outline-container-org531c508" class="outline-2">
<h2 id="org531c508">激活函数</h2>
<div class="outline-text-2" id="text-org531c508">
<pre class="example">
    使用一个神经网络时，需要决定使用哪种激活函数用隐藏层上，哪种用在输出节点上

    到目前为止，之前的视频只用过sigmoid激活函数，但是，有时其他的激活函数效果会更好
</pre>

<p>
在神经网路的前向传播中，的 \(a^{[1]} = \sigma(z^{[1]})\) 和 \(a^{[2]} = \sigma(z^{[2]})\) 这两步会使用到 \(\mathbf{sigmoid}\) 函数。\(\mathbf{sigmoid}\) 函数在这里被称为 <b>激活函数</b> ：
</p>

\begin{equation}
a = \sigma(z) = \frac{1}{1 + e^{-z}}
\end{equation} 

<p>
更通常的情况下，使用不同的函数 \(g(z^{[1]})\) ， \(g\) 可以是除了\(\mathbf{sigmoid}\) 函数以外的非线性函数： \(\mathbf{tanh}\) 函数或者双曲正切函数总体上是都优于 \(\mathbf{sigmoid}\) 函数的激活函数
</p>

\begin{equation}
a = \mathbf{tanh}(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}
\end{equation} 

<p>
事实上，\(\mathbf{tanh}\) 函数是 \(sigmoid\) 的向下平移和伸缩后的结果。对它进行了变形后，穿过了点，并且值域介于+1和-1之间。结果表明，如果在隐藏层上使用 \(tanh\) 函数效果总是优于 \(sigmoid\) 函数。因为函数值域在-1和+1的激活函数，其均值是更接近零均值的。在训练一个算法模型时，如果使用tanh函数代替sigmoid函数中心化数据，使得数据的平均值更接近0而不是0.5 
</p>

<pre class="example">
    在讨论优化算法时，有一点要说明：基本已经不用sigmoid激活函数了，tanh函数在所有场合都优于sigmoid函数。

    但有一个例外：在二分类的问题中，对于输出层，因为的值是0或1，所以想让的数值介于0和1之间，而不是在-1和+1之间。所以需要使用sigmoid激活函数

    在这个例子里看到的是，对隐藏层使用tanh激活函数，输出层使用sigmoid函数
</pre>

<p>
在不同的神经网络层中，激活函数可以不同。为了表示不同的激活函数，在不同的层中，使用 <span class="underline">方括号上标</span> 来指出 \(g\) 上标为 \([1]\) 的激活函数，可能会跟 \(g\) 上标 \([2]\) 不同。方括号上标 \([1]\) 代表 <b>隐藏层</b> ，方括号上标 \([2]\) 表示 <b>输出层</b> 
</p>

<pre class="example">
sigmoid函数和tanh函数两者共同的缺点：

在z特别大或者特别小的情况下，导数的梯度或者函数的斜率会变得特别小

最后就会接近于0，导致降低梯度下降的速度
</pre>

<p>
在机器学习另一个很流行的函数是： <b>修正线性单元</b> 的函数 \(\mathbf{ReLu}\) ： 
</p>
\begin{equation}
a = max(0, z)  
\end{equation}

<ul class="org-ul">
<li>只要 \(z\) 是正值的情况下，导数恒等于1</li>
<li>当 \(z\) 是负值的时候，导数恒等于0</li>
<li><p>
从实际上来说，当使用的导数时， \(z=0\) 的导数是没有定义的
</p>
<pre class="example">
      但是当编程实现的时候，z的取值刚好等于0.00000001，这个值相当小

      所以，在实践中，不需要担心这个值，z是等于0的时候，假设一个导数是1或者0效果都可以
</pre></li>
</ul>

<p>
一些选择激活函数的经验法则：
</p>
<ul class="org-ul">
<li>如果输出是0和1值，则输出层选择 \(sigmoid\) 函数，然后其它的所有单元都选择 \(Relu\) 函数</li>
<li>如果在隐藏层上不确定使用哪个激活函数，那么通常会使用 \(Relu\) 激活函数
<ul class="org-ul">
<li>有时，也会使用 \(tanh\) 激活函数，但 \(Relu\) 的一个优点是： <b>当z是负值的时候，导数等于0</b></li>
</ul></li>
</ul>

<p>
也有另一个版本的Relu被称为 \(\mathbf{Leaky Relu}\) ，当 \(z\) 是负值时，这个函数的值不是等于0，而是轻微的倾斜：
</p>

\begin{equation}
a = max(0.01z, z)  
\end{equation}

<pre class="example">
    为什么常数是0.01？当然，可以为学习算法选择不同的参数
</pre>


<div class="figure">
<p><img src="../pic/L1_week3_9.jpg" alt="L1_week3_9.jpg" width="50%" /> 
</p>
</div>

<pre class="example">
    这个函数通常比Relu激活函数效果要好，尽管在实际中Leaky ReLu使用的并不多
</pre>

<p>
两者的优点是：
</p>
<ol class="org-ol">
<li><p>
在 \(z\) 的区间变动很大的情况下，激活函数的导数或者激活函数的斜率都会远大于0。在实践中，使用 \(ReLu\) 激活函数神经网络通常会比使用 \(sigmoid\) 或者 \(tanh\) 激活函数学习的更快
</p>
<pre class="example">
       在程序实现 Relu 就是一个if-else语句，而sigmoid函数需要进行浮点四则运算
</pre></li>
<li>\(sigmoid\) 和 \(tanh\) 函数的导数在正负饱和区的梯度都会接近于0，这会造成 <b>梯度弥散</b> ，而 \(Relu\) 和 \(Leaky ReLu\) 函数大于0部分都为常数，不会产生梯度弥散现象。同时应该注意到的是：
<ul class="org-ul">
<li>\(Relu\) 进入负半区的时候，梯度为0，神经元此时不会训练，产生所谓的稀疏性</li>
<li><p>
\(Leaky ReLu\) 不会有这问题
</p>
<pre class="example">
	 尽管ReLu的梯度一半都是0，但是，有足够的隐藏层使得z值大于0，所以对大多数的训练数据来说学习过程仍然可以很快
</pre></li>
</ul></li>
</ol>
</div>

<div id="outline-container-org3414fc0" class="outline-3">
<h3 id="org3414fc0">总结</h3>
<div class="outline-text-3" id="text-org3414fc0">
<ul class="org-ul">
<li>\(\mathbf{sigmoid}\) 激活函数：除了输出层是一个二分类问题基本不会用它</li>
<li>\(\mathbf{tanh}\) 激活函数： \(tanh\) 是非常优秀的，几乎适合所有场合</li>
<li><p>
\(\mathbf{ReLu}\) 激活函数：最常用的默认函数
</p>
<pre class="example">
       如果不确定用哪个激活函数，就使用ReLu或者Leaky ReLu
</pre></li>
</ul>

<p>
在选择自己神经网络的激活函数时，有一定的直观感受法
</p>

<pre class="example">
     在深度学习中的经常遇到一个问题：在编写神经网络的时候，会有很多选择
     隐藏层单元的个数、激活函数的选择、初始化权值……
     这些选择想得到一个对比较好的指导原则是挺困难的


     这里只提供一种直观的感受，哪一种工业界用的多，哪一种用的少
     但是，自己的神经网络的应用，以及其特殊性，是很难提前知道选择哪些效果更好
     所以通常的建议是：如果不确定哪一个激活函数效果更好，可以把它们都试试
     然后在验证集或者发展集上进行评价看哪一种表现的更好，就去使用它
</pre>
</div>
</div>
</div>

<div id="outline-container-orgd2886c4" class="outline-2">
<h2 id="orgd2886c4">为什么需要非线性激活函数?</h2>
<div class="outline-text-2" id="text-orgd2886c4">
<pre class="example">
    为什么神经网络需要非线性激活函数？

    事实证明：要让你的神经网络能够计算出有趣的函数，就必须使用非线性激活函数
</pre>

<p>
在神经网络正向传播的方程，现在去掉函数 \(g\) ，然后令 \(a^{[1]} = z^{[1]}\) ，或者也可以令 \(g(z) = z\) 
</p>
<pre class="example">
    这个有时被叫做线性激活函数，更学术点的名字是恒等激励函数
</pre>

<p>
为了说明问题把 \(a^{[2]} = z^{[2]}\) ，那么这个模型的输出 \(y\) 仅仅只是输入特征 \(x\) 的线性组合
</p>

<p>
改变前面的方程，令
</p>

\begin{equation} 
a^{[1]} = z^{[1]} = W^{[1]}x + b^{[1]} 
\end{equation} 

\begin{equation} 
a^{[2]} = z^{[2]} = W^{[2]}a^{[1]} + b^{[2]} 
\end{equation} 

<p>
则可以得出：
</p>

\begin{equation}
a^{[2]} = z^{[2]} = W^{[2]}(W^{[1]}x + b^{[1]}) + b^{[2]} = W^{[2]}W^{[1]}x + W^{[2]}b^{[1]} + b^{[2]} = W^{'}x + b^{'}
\end{equation}

<p>
如果是用线性激活函数或者叫恒等激励函数，那么神经网络只是把输入线性组合再输出
</p>

<pre class="example">
    稍后学到的深度网络，有很多层的神经网络，很多隐藏层

    事实证明，如果使用线性激活函数或者没有使用一个激活函数，那么无论神经网络有多少层一直在做的只是计算线性函数

    所以不如直接去掉全部隐藏层

    在我们的简单案例中，如果在隐藏层用线性激活函数，在输出层用sigmoid函数

    那么这个模型的复杂度和没有任何隐藏层的标准Logistic回归是一样的z
</pre>

<p>
在这里线性隐层一点用也没有，因为这两个线性函数的组合本身就是线性函数，所以除非引入非线性，否则无法计算更有趣的函数，即使网络层数再多也不行；只有一个地方可以使用线性激活函数就是在做机器学习中的回归问题
</p>

<pre class="example">
    举个例子，比如想预测房地产价格，这就不是二分类任务0或1，而是一个实数，从0到正无穷

    如果y是个实数，那么在输出层用线性激活函数也许可行，输出也是一个实数，从负无穷到正无穷
</pre>

<p>
总而言之，不能在隐藏层用线性激活函数，可以用 \(ReLU\) 或者 \(tanh\) 或者 \(leaky ReLU\) 或者其他的非线性激活函数，唯一可以用线性激活函数的通常就是输出层
</p>

<pre class="example">
    理解为什么使用非线性激活函数对于神经网络十分关键，接下来讨论梯度下降
</pre>
</div>
</div>

<div id="outline-container-org0410b46" class="outline-2">
<h2 id="org0410b46">激活函数的导数</h2>
<div class="outline-text-2" id="text-org0410b46">
<p>
在神经网络中使用反向传播的时候，需要计算激活函数的斜率或者导数。针对以下四种激活，求其导数如下
</p>
</div>

<div id="outline-container-org47377f3" class="outline-3">
<h3 id="org47377f3">sigmod</h3>
<div class="outline-text-3" id="text-org47377f3">

<div class="figure">
<p><img src="../pic/L1_week3_10.png" alt="L1_week3_10.png" width="50%" /> 
</p>
</div>

\begin{equation}
\frac{\mathrm{d} g(z)}{\mathrm{d} z} = \frac{1}{1 + e^{-z}}(1 - \frac{1}{1 + e^{-z}}) = g(z)(1-g(z))  
\end{equation} 

<p>
在神经网络中： \(a = g(z) \Longrightarrow g(z)^{'} = \frac{\mathrm{d} g(z)}{\mathrm{d} z} = a(1-a)\) . 注意：
</p>
<ul class="org-ul">
<li>\(当z = 1 或 z = -1: \frac{\mathrm{d} g(z)}{\mathrm{d} z} \approx 0\)</li>
<li>\(当z = 0: \frac{\mathrm{d} g(z)}{\mathrm{d} z} = \frac{1}{4}\)</li>
</ul>
</div>
</div>

<div id="outline-container-org6cff32c" class="outline-3">
<h3 id="org6cff32c">tanh</h3>
<div class="outline-text-3" id="text-org6cff32c">

<div class="figure">
<p><img src="../pic/L1_week3_11.png" alt="L1_week3_11.png" width="50%" /> 
</p>
</div>

\begin{equation}
 \frac{\mathrm{d} g(z)}{\mathrm{d} z} = 1 - (tanh(z))^{2} 
\end{equation}

<p>
在神经网络中： \(g(z)^{'} = 1 - a^{2}\) 
</p>
<ul class="org-ul">
<li>\(当z = 1 或 z = -1: \frac{\mathrm{d} g(z)}{\mathrm{d} z} \approx 0\)</li>
<li>\(当z = 0: \frac{\mathrm{d} g(z)}{\mathrm{d} z} = 1 - 0^2 = 1\)</li>
</ul>
</div>
</div>

<div id="outline-container-orgbeefd36" class="outline-3">
<h3 id="orgbeefd36">Relu</h3>
<div class="outline-text-3" id="text-orgbeefd36">

<div class="figure">
<p><img src="../pic/L1_week3_12.png" alt="L1_week3_12.png" width="50%" /> 
</p>
</div>

\begin{equation}
g(z)^{'} = \left\{
  \begin{array}{c}
  0 & if \quad z < 0 \\ 
  1 & if \quad z > 0 \\ 
  undefined & if \quad z = 0
  \end{array}
  \right.  
\end{equation}

<p>
注: 通常在 \(z=0\) 的时候给定其导数1 ；当然 \(z=0\) 的情况很少
</p>
</div>
</div>

<div id="outline-container-org9c45b6c" class="outline-3">
<h3 id="org9c45b6c">Leaky Relu</h3>
<div class="outline-text-3" id="text-org9c45b6c">
<p>
与 \(Relu\) 类似：
</p>

\begin{equation}
g(z)^{'} = \left\{
  \begin{array}{c}
  0.01 & if \quad z < 0 \\
  1 & if \quad z > 0 \\
  undefined & if \quad z = 0
  \end{array}
  \right.
\end{equation}

<p>
注: 通常在 \(z=0\) 的时候给定其导数1 ；当然 \(z=0\) 的情况很少
</p>
</div>
</div>
</div>

<div id="outline-container-orgc436ab0" class="outline-2">
<h2 id="orgc436ab0">神经网络的梯度下降</h2>
</div>
</div>
<div id="postamble" class="status">

		  <br/>
		  <div class='ds-thread'></div>
		  <script>
		  var duoshuoQuery = {short_name:'klose911'};
		  (function() {
					  var dsThread = document.getElementsByClassName('ds-thread')[0];
					  dsThread.setAttribute('data-thread-key', document.title);
					  dsThread.setAttribute('data-title', document.title);
					  dsThread.setAttribute('data-url', window.location.href);
					  var ds = document.createElement('script');
					  ds.type = 'text/javascript';ds.async = true;
					  ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
					  ds.charset = 'UTF-8';
					  (document.getElementsByTagName('head')[0] 
						|| document.getElementsByTagName('body')[0]).appendChild(ds);
					  })();
		  </script>
		  <script>
		  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
			(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
			m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
			})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
		  ga('create', 'UA-90850421-1', 'auto');
		  ga('send', 'pageview');
		  </script>
</div>
</body>
</html>
